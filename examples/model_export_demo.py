"""
æ¨¡å‹å¯¼å‡ºç¤ºä¾‹

æ¼”ç¤ºå¦‚ä½•å°† PyTorch æ¨¡å‹å¯¼å‡ºä¸º ONNX å’Œ TorchScript æ ¼å¼ã€‚
"""

import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

import torch
import torch.nn as nn


def demo_simple_export():
    """æ¼”ç¤ºç®€å•æ¨¡å‹å¯¼å‡º"""
    print("=" * 60)
    print("ç®€å•æ¨¡å‹å¯¼å‡ºæ¼”ç¤º")
    print("=" * 60)
    
    from med_core.utils.export import ModelExporter
    
    # åˆ›å»ºä¸€ä¸ªç®€å•çš„æ¨¡å‹
    class SimpleModel(nn.Module):
        def __init__(self):
            super().__init__()
            self.conv = nn.Conv2d(3, 64, 3, padding=1)
            self.relu = nn.ReLU()
            self.pool = nn.AdaptiveAvgPool2d(1)
            self.fc = nn.Linear(64, 10)
        
        def forward(self, x):
            x = self.conv(x)
            x = self.relu(x)
            x = self.pool(x)
            x = x.flatten(1)
            x = self.fc(x)
            return x
    
    model = SimpleModel()
    
    # åˆ›å»ºå¯¼å‡ºå™¨
    exporter = ModelExporter(
        model=model,
        input_shape=(3, 224, 224),
        device="cpu",
    )
    
    # å¯¼å‡ºä¸º ONNX
    print("\n1. å¯¼å‡ºä¸º ONNX:")
    exporter.export_onnx(
        "outputs/simple_model.onnx",
        opset_version=11,
        input_names=["image"],
        output_names=["logits"],
    )
    
    # éªŒè¯ ONNX æ¨¡å‹
    print("\n2. éªŒè¯ ONNX æ¨¡å‹:")
    exporter.verify_onnx("outputs/simple_model.onnx")
    
    # å¯¼å‡ºä¸º TorchScript (trace)
    print("\n3. å¯¼å‡ºä¸º TorchScript (trace):")
    exporter.export_torchscript(
        "outputs/simple_model_trace.pt",
        method="trace",
        optimize=True,
    )
    
    # éªŒè¯ TorchScript æ¨¡å‹
    print("\n4. éªŒè¯ TorchScript æ¨¡å‹:")
    exporter.verify_torchscript("outputs/simple_model_trace.pt")
    
    # å¯¼å‡ºä¸º TorchScript (script)
    print("\n5. å¯¼å‡ºä¸º TorchScript (script):")
    exporter.export_torchscript(
        "outputs/simple_model_script.pt",
        method="script",
        optimize=True,
    )


def demo_multimodal_export():
    """æ¼”ç¤ºå¤šæ¨¡æ€æ¨¡å‹å¯¼å‡º"""
    print("\n" + "=" * 60)
    print("å¤šæ¨¡æ€æ¨¡å‹å¯¼å‡ºæ¼”ç¤º")
    print("=" * 60)
    
    from med_core.utils.export import MultiModalExporter
    
    # åˆ›å»ºä¸€ä¸ªå¤šæ¨¡æ€æ¨¡å‹
    class MultiModalModel(nn.Module):
        def __init__(self):
            super().__init__()
            # å›¾åƒåˆ†æ”¯
            self.image_encoder = nn.Sequential(
                nn.Conv2d(3, 64, 3, padding=1),
                nn.ReLU(),
                nn.AdaptiveAvgPool2d(1),
                nn.Flatten(),
            )
            # è¡¨æ ¼åˆ†æ”¯
            self.tabular_encoder = nn.Sequential(
                nn.Linear(10, 64),
                nn.ReLU(),
            )
            # èåˆ
            self.fusion = nn.Linear(128, 10)
        
        def forward(self, image, tabular):
            image_feat = self.image_encoder(image)
            tabular_feat = self.tabular_encoder(tabular)
            fused = torch.cat([image_feat, tabular_feat], dim=1)
            output = self.fusion(fused)
            return output
    
    model = MultiModalModel()
    
    # åˆ›å»ºå¤šæ¨¡æ€å¯¼å‡ºå™¨
    exporter = MultiModalExporter(
        model=model,
        input_shapes={
            "image": (3, 224, 224),
            "tabular": (10,),
        },
        device="cpu",
    )
    
    # å¯¼å‡ºä¸º ONNX
    print("\n1. å¯¼å‡ºä¸º ONNX:")
    exporter.export_onnx(
        "outputs/multimodal_model.onnx",
        input_names=["image", "tabular"],
        output_names=["logits"],
    )
    
    # å¯¼å‡ºä¸º TorchScript
    print("\n2. å¯¼å‡ºä¸º TorchScript:")
    exporter.export_torchscript(
        "outputs/multimodal_model.pt",
        method="trace",
    )


def demo_convenience_function():
    """æ¼”ç¤ºä¾¿æ·å‡½æ•°"""
    print("\n" + "=" * 60)
    print("ä¾¿æ·å‡½æ•°æ¼”ç¤º")
    print("=" * 60)
    
    from med_core.utils.export import export_model
    
    # åˆ›å»ºæ¨¡å‹
    model = nn.Sequential(
        nn.Conv2d(3, 64, 3, padding=1),
        nn.ReLU(),
        nn.AdaptiveAvgPool2d(1),
        nn.Flatten(),
        nn.Linear(64, 10),
    )
    
    # ä½¿ç”¨ä¾¿æ·å‡½æ•°å¯¼å‡º
    print("\n1. å¯¼å‡ºä¸º ONNX:")
    export_model(
        model=model,
        output_path="outputs/model_convenience.onnx",
        input_shape=(3, 224, 224),
        format="onnx",
        verify=True,
    )
    
    print("\n2. å¯¼å‡ºä¸º TorchScript:")
    export_model(
        model=model,
        output_path="outputs/model_convenience.pt",
        input_shape=(3, 224, 224),
        format="torchscript",
        verify=True,
    )


def demo_dynamic_axes():
    """æ¼”ç¤ºåŠ¨æ€è½´"""
    print("\n" + "=" * 60)
    print("åŠ¨æ€è½´æ¼”ç¤º")
    print("=" * 60)
    
    from med_core.utils.export import ModelExporter
    
    # åˆ›å»ºæ¨¡å‹
    model = nn.Sequential(
        nn.Conv2d(3, 64, 3, padding=1),
        nn.ReLU(),
        nn.AdaptiveAvgPool2d(1),
        nn.Flatten(),
        nn.Linear(64, 10),
    )
    
    exporter = ModelExporter(model, input_shape=(3, 224, 224))
    
    # å¯¼å‡ºæ—¶æŒ‡å®šåŠ¨æ€è½´
    print("\nå¯¼å‡ºæ”¯æŒåŠ¨æ€ batch size å’Œå›¾åƒå°ºå¯¸çš„æ¨¡å‹:")
    exporter.export_onnx(
        "outputs/model_dynamic.onnx",
        input_names=["image"],
        output_names=["logits"],
        dynamic_axes={
            "image": {
                0: "batch_size",
                2: "height",
                3: "width",
            },
            "logits": {0: "batch_size"},
        },
    )
    
    print("\nâœ“ æ¨¡å‹æ”¯æŒ:")
    print("  â€¢ åŠ¨æ€ batch size")
    print("  â€¢ åŠ¨æ€å›¾åƒé«˜åº¦")
    print("  â€¢ åŠ¨æ€å›¾åƒå®½åº¦")


def demo_inference():
    """æ¼”ç¤ºæ¨ç†"""
    print("\n" + "=" * 60)
    print("æ¨ç†æ¼”ç¤º")
    print("=" * 60)
    
    # åˆ›å»ºå¹¶å¯¼å‡ºæ¨¡å‹
    model = nn.Sequential(
        nn.Conv2d(3, 64, 3, padding=1),
        nn.ReLU(),
        nn.AdaptiveAvgPool2d(1),
        nn.Flatten(),
        nn.Linear(64, 10),
    )
    
    from med_core.utils.export import ModelExporter
    
    exporter = ModelExporter(model, input_shape=(3, 224, 224))
    exporter.export_onnx("outputs/model_inference.onnx")
    exporter.export_torchscript("outputs/model_inference.pt")
    
    # 1. PyTorch æ¨ç†
    print("\n1. PyTorch æ¨ç†:")
    model.eval()
    x = torch.randn(1, 3, 224, 224)
    with torch.no_grad():
        pytorch_output = model(x)
    print(f"   è¾“å‡ºå½¢çŠ¶: {pytorch_output.shape}")
    print(f"   è¾“å‡ºèŒƒå›´: [{pytorch_output.min():.3f}, {pytorch_output.max():.3f}]")
    
    # 2. TorchScript æ¨ç†
    print("\n2. TorchScript æ¨ç†:")
    loaded_model = torch.jit.load("outputs/model_inference.pt")
    loaded_model.eval()
    with torch.no_grad():
        torchscript_output = loaded_model(x)
    print(f"   è¾“å‡ºå½¢çŠ¶: {torchscript_output.shape}")
    print(f"   ä¸ PyTorch çš„å·®å¼‚: {(pytorch_output - torchscript_output).abs().max():.6f}")
    
    # 3. ONNX æ¨ç†
    print("\n3. ONNX æ¨ç†:")
    try:
        import onnxruntime as ort
        
        ort_session = ort.InferenceSession("outputs/model_inference.onnx")
        ort_inputs = {ort_session.get_inputs()[0].name: x.numpy()}
        ort_output = ort_session.run(None, ort_inputs)[0]
        
        print(f"   è¾“å‡ºå½¢çŠ¶: {ort_output.shape}")
        print(f"   ä¸ PyTorch çš„å·®å¼‚: {abs(pytorch_output.numpy() - ort_output).max():.6f}")
    except ImportError:
        print("   âš  ONNXRuntime æœªå®‰è£…ï¼Œè·³è¿‡ ONNX æ¨ç†")


def demo_best_practices():
    """æ¼”ç¤ºæœ€ä½³å®è·µ"""
    print("\n" + "=" * 60)
    print("æœ€ä½³å®è·µ")
    print("=" * 60)
    
    print("\n1. é€‰æ‹©åˆé€‚çš„å¯¼å‡ºæ ¼å¼:")
    print("   â€¢ ONNX: è·¨å¹³å°éƒ¨ç½²ï¼Œæ”¯æŒå¤šç§æ¨ç†å¼•æ“")
    print("   â€¢ TorchScript: PyTorch ç”Ÿæ€ï¼Œæ€§èƒ½ä¼˜åŒ–")
    
    print("\n2. å¯¼å‡ºå‰çš„å‡†å¤‡:")
    print("   â€¢ è®¾ç½®æ¨¡å‹ä¸ºè¯„ä¼°æ¨¡å¼ (model.eval())")
    print("   â€¢ ç§»é™¤è®­ç»ƒç›¸å…³çš„æ“ä½œï¼ˆdropoutã€batch normï¼‰")
    print("   â€¢ æµ‹è¯•æ¨¡å‹åœ¨ä¸åŒè¾“å…¥ä¸‹çš„è¡Œä¸º")
    
    print("\n3. éªŒè¯å¯¼å‡ºçš„æ¨¡å‹:")
    print("   â€¢ æ¯”è¾ƒè¾“å‡ºæ˜¯å¦ä¸€è‡´")
    print("   â€¢ æµ‹è¯•ä¸åŒçš„è¾“å…¥å°ºå¯¸ï¼ˆå¦‚æœä½¿ç”¨åŠ¨æ€è½´ï¼‰")
    print("   â€¢ æµ‹è¯•è¾¹ç•Œæƒ…å†µ")
    
    print("\n4. ä¼˜åŒ–å»ºè®®:")
    print("   â€¢ ä½¿ç”¨ optimize_for_inference (TorchScript)")
    print("   â€¢ é€‰æ‹©åˆé€‚çš„ opset ç‰ˆæœ¬ (ONNX)")
    print("   â€¢ è€ƒè™‘é‡åŒ–å’Œå‰ªæ")
    
    print("\n5. å¸¸è§é—®é¢˜:")
    print("   â€¢ åŠ¨æ€æ§åˆ¶æµ: ä½¿ç”¨ script è€Œä¸æ˜¯ trace")
    print("   â€¢ è‡ªå®šä¹‰ç®—å­: éœ€è¦æ³¨å†Œ ONNX ç®—å­")
    print("   â€¢ ç‰ˆæœ¬å…¼å®¹æ€§: æ³¨æ„ PyTorch å’Œ ONNX ç‰ˆæœ¬")


def main():
    """ä¸»å‡½æ•°"""
    print("\n" + "=" * 60)
    print("MedFusion æ¨¡å‹å¯¼å‡ºæ¼”ç¤º")
    print("=" * 60)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    Path("outputs").mkdir(exist_ok=True)
    
    try:
        # æ¼”ç¤º 1: ç®€å•æ¨¡å‹å¯¼å‡º
        demo_simple_export()
        
        # æ¼”ç¤º 2: å¤šæ¨¡æ€æ¨¡å‹å¯¼å‡º
        demo_multimodal_export()
        
        # æ¼”ç¤º 3: ä¾¿æ·å‡½æ•°
        demo_convenience_function()
        
        # æ¼”ç¤º 4: åŠ¨æ€è½´
        demo_dynamic_axes()
        
        # æ¼”ç¤º 5: æ¨ç†
        demo_inference()
        
        # æ¼”ç¤º 6: æœ€ä½³å®è·µ
        demo_best_practices()
        
        print("\n" + "=" * 60)
        print("æ¼”ç¤ºå®Œæˆï¼")
        print("=" * 60)
        
        print("\nğŸ’¡ å…³é”®è¦ç‚¹:")
        print("  1. ONNX é€‚åˆè·¨å¹³å°éƒ¨ç½²")
        print("  2. TorchScript é€‚åˆ PyTorch ç”Ÿæ€")
        print("  3. å§‹ç»ˆéªŒè¯å¯¼å‡ºçš„æ¨¡å‹")
        print("  4. ä½¿ç”¨åŠ¨æ€è½´æ”¯æŒä¸åŒè¾“å…¥å°ºå¯¸")
        print("  5. ä¼˜åŒ–æ¨¡å‹ä»¥æé«˜æ¨ç†æ€§èƒ½")
        
        print("\nğŸ“– ç›¸å…³èµ„æº:")
        print("  â€¢ med_core/utils/export.py")
        print("  â€¢ examples/model_export_demo.py")
        print("  â€¢ docs/guides/model_export.md")
        
    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
