"""
é«˜çº§æ³¨æ„åŠ›æ¨¡å—ä½¿ç”¨ç¤ºä¾‹

æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨ SEã€ECAã€Transformer ç­‰æ³¨æ„åŠ›æœºåˆ¶ã€‚
"""

import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

import torch
import torch.nn as nn


def demo_se_attention():
    """æ¼”ç¤º SE æ³¨æ„åŠ›"""
    print("=" * 60)
    print("SE (Squeeze-and-Excitation) æ³¨æ„åŠ›æ¼”ç¤º")
    print("=" * 60)
    
    from med_core.attention_supervision import SEAttention
    
    # åˆ›å»º SE æ¨¡å—
    se = SEAttention(channels=256, reduction=16)
    
    # è¾“å…¥ç‰¹å¾
    x = torch.randn(2, 256, 14, 14)
    print(f"\nè¾“å…¥å½¢çŠ¶: {x.shape}")
    
    # åº”ç”¨ SE æ³¨æ„åŠ›
    out = se(x)
    print(f"è¾“å‡ºå½¢çŠ¶: {out.shape}")
    
    # è·å–æ³¨æ„åŠ›æƒé‡
    weights = se.get_attention_weights(x)
    print(f"é€šé“æ³¨æ„åŠ›æƒé‡å½¢çŠ¶: {weights.shape}")
    print(f"æƒé‡èŒƒå›´: [{weights.min():.3f}, {weights.max():.3f}]")
    
    print("\nç‰¹ç‚¹:")
    print("  â€¢ é€šé“æ³¨æ„åŠ›æœºåˆ¶")
    print("  â€¢ å…¨å±€å¹³å‡æ± åŒ– + ä¸¤å±‚å…¨è¿æ¥")
    print("  â€¢ å‚æ•°é‡å°ï¼Œè®¡ç®—é«˜æ•ˆ")
    print("  â€¢ é€‚åˆå¢å¼ºé‡è¦é€šé“")


def demo_eca_attention():
    """æ¼”ç¤º ECA æ³¨æ„åŠ›"""
    print("\n" + "=" * 60)
    print("ECA (Efficient Channel Attention) æ³¨æ„åŠ›æ¼”ç¤º")
    print("=" * 60)
    
    from med_core.attention_supervision import ECAAttention
    
    # åˆ›å»º ECA æ¨¡å—
    eca = ECAAttention(channels=256)
    
    # è¾“å…¥ç‰¹å¾
    x = torch.randn(2, 256, 14, 14)
    print(f"\nè¾“å…¥å½¢çŠ¶: {x.shape}")
    
    # åº”ç”¨ ECA æ³¨æ„åŠ›
    out = eca(x)
    print(f"è¾“å‡ºå½¢çŠ¶: {out.shape}")
    
    # è·å–æ³¨æ„åŠ›æƒé‡
    weights = eca.get_attention_weights(x)
    print(f"é€šé“æ³¨æ„åŠ›æƒé‡å½¢çŠ¶: {weights.shape}")
    
    print(f"\nè‡ªåŠ¨è®¡ç®—çš„å·ç§¯æ ¸å¤§å°: {eca.kernel_size}")
    
    print("\nç‰¹ç‚¹:")
    print("  â€¢ é«˜æ•ˆçš„é€šé“æ³¨æ„åŠ›")
    print("  â€¢ ä½¿ç”¨ 1D å·ç§¯ï¼Œé¿å…é™ç»´")
    print("  â€¢ å‚æ•°é‡æ›´å°‘")
    print("  â€¢ æ€§èƒ½ä¼˜äº SE")


def demo_spatial_attention():
    """æ¼”ç¤ºç©ºé—´æ³¨æ„åŠ›"""
    print("\n" + "=" * 60)
    print("ç©ºé—´æ³¨æ„åŠ›æ¼”ç¤º")
    print("=" * 60)
    
    from med_core.attention_supervision import SpatialAttention
    
    # åˆ›å»ºç©ºé—´æ³¨æ„åŠ›æ¨¡å—
    sa = SpatialAttention(kernel_size=7)
    
    # è¾“å…¥ç‰¹å¾
    x = torch.randn(2, 256, 14, 14)
    print(f"\nè¾“å…¥å½¢çŠ¶: {x.shape}")
    
    # åº”ç”¨ç©ºé—´æ³¨æ„åŠ›
    out = sa(x)
    print(f"è¾“å‡ºå½¢çŠ¶: {out.shape}")
    
    # è·å–æ³¨æ„åŠ›æƒé‡
    weights = sa.get_attention_weights(x)
    print(f"ç©ºé—´æ³¨æ„åŠ›æƒé‡å½¢çŠ¶: {weights.shape}")
    
    print("\nç‰¹ç‚¹:")
    print("  â€¢ ç©ºé—´ç»´åº¦çš„æ³¨æ„åŠ›")
    print("  â€¢ ä½¿ç”¨å¹³å‡æ± åŒ–å’Œæœ€å¤§æ± åŒ–")
    print("  â€¢ å…³æ³¨é‡è¦çš„ç©ºé—´ä½ç½®")
    print("  â€¢ é€‚åˆç›®æ ‡å®šä½")


def demo_cbam():
    """æ¼”ç¤º CBAM"""
    print("\n" + "=" * 60)
    print("CBAM (Convolutional Block Attention Module) æ¼”ç¤º")
    print("=" * 60)
    
    from med_core.attention_supervision import CBAM
    
    # åˆ›å»º CBAM æ¨¡å—
    cbam = CBAM(channels=256, reduction=16, spatial_kernel=7)
    
    # è¾“å…¥ç‰¹å¾
    x = torch.randn(2, 256, 14, 14)
    print(f"\nè¾“å…¥å½¢çŠ¶: {x.shape}")
    
    # åº”ç”¨ CBAM
    out = cbam(x)
    print(f"è¾“å‡ºå½¢çŠ¶: {out.shape}")
    
    print("\nç‰¹ç‚¹:")
    print("  â€¢ ç»“åˆé€šé“å’Œç©ºé—´æ³¨æ„åŠ›")
    print("  â€¢ å…ˆé€šé“åç©ºé—´")
    print("  â€¢ æ€§èƒ½å¼ºå¤§")
    print("  â€¢ å¹¿æ³›åº”ç”¨äºå„ç§ä»»åŠ¡")


def demo_transformer_attention():
    """æ¼”ç¤º Transformer æ³¨æ„åŠ›"""
    print("\n" + "=" * 60)
    print("Transformer æ³¨æ„åŠ›æ¼”ç¤º")
    print("=" * 60)
    
    from med_core.attention_supervision import TransformerAttention2D
    
    # åˆ›å»º Transformer æ³¨æ„åŠ›æ¨¡å—
    ta = TransformerAttention2D(channels=256, num_heads=8)
    
    # è¾“å…¥ç‰¹å¾
    x = torch.randn(2, 256, 14, 14)
    print(f"\nè¾“å…¥å½¢çŠ¶: {x.shape}")
    
    # åº”ç”¨ Transformer æ³¨æ„åŠ›
    out = ta(x)
    print(f"è¾“å‡ºå½¢çŠ¶: {out.shape}")
    
    # è·å–æ³¨æ„åŠ›æƒé‡
    weights = ta.get_attention_weights(x)
    print(f"æ³¨æ„åŠ›æƒé‡å½¢çŠ¶: {weights.shape}")
    print(f"  (B, num_heads, N, N) = (batch, å¤´æ•°, åºåˆ—é•¿åº¦, åºåˆ—é•¿åº¦)")
    
    print("\nç‰¹ç‚¹:")
    print("  â€¢ å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶")
    print("  â€¢ å…¨å±€æ„Ÿå—é‡")
    print("  â€¢ æ•è·é•¿è·ç¦»ä¾èµ–")
    print("  â€¢ è®¡ç®—å¤æ‚åº¦ O(NÂ²)")


def demo_factory_function():
    """æ¼”ç¤ºå·¥å‚å‡½æ•°"""
    print("\n" + "=" * 60)
    print("å·¥å‚å‡½æ•°æ¼”ç¤º")
    print("=" * 60)
    
    from med_core.attention_supervision import create_attention_module
    
    print("\næ”¯æŒçš„æ³¨æ„åŠ›ç±»å‹:")
    attention_types = ["se", "eca", "spatial", "cbam", "transformer"]
    
    for attn_type in attention_types:
        attn = create_attention_module(attn_type, channels=256)
        x = torch.randn(2, 256, 14, 14)
        out = attn(x)
        print(f"  â€¢ {attn_type:12s}: {x.shape} -> {out.shape}")
    
    print("\nä½¿ç”¨ç¤ºä¾‹:")
    code = '''
from med_core.attention_supervision import create_attention_module

# åˆ›å»ºæ³¨æ„åŠ›æ¨¡å—
attention = create_attention_module(
    attention_type="se",
    channels=256,
    reduction=16,
)

# åœ¨æ¨¡å‹ä¸­ä½¿ç”¨
class MyModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv = nn.Conv2d(256, 256, 3, padding=1)
        self.attention = create_attention_module("se", channels=256)
    
    def forward(self, x):
        x = self.conv(x)
        x = self.attention(x)  # åº”ç”¨æ³¨æ„åŠ›
        return x
'''
    print(code)


def demo_attention_supervision():
    """æ¼”ç¤ºæ³¨æ„åŠ›ç›‘ç£"""
    print("\n" + "=" * 60)
    print("æ³¨æ„åŠ›ç›‘ç£æ¼”ç¤º")
    print("=" * 60)
    
    from med_core.attention_supervision import (
        ChannelAttentionSupervision,
        SpatialAttentionSupervision,
        TransformerAttentionSupervision,
    )
    
    print("\n1. é€šé“æ³¨æ„åŠ›ç›‘ç£:")
    channel_sup = ChannelAttentionSupervision(
        loss_weight=0.1,
        diversity_weight=0.1,
        sparsity_weight=0.1,
    )
    
    channel_weights = torch.sigmoid(torch.randn(2, 256))
    features = torch.randn(2, 256, 14, 14)
    
    loss = channel_sup(channel_weights, features)
    print(f"   æ€»æŸå¤±: {loss.total_loss.item():.4f}")
    print(f"   æŸå¤±ç»„ä»¶: {list(loss.components.keys())}")
    
    print("\n2. ç©ºé—´æ³¨æ„åŠ›ç›‘ç£:")
    spatial_sup = SpatialAttentionSupervision(
        loss_weight=0.1,
        consistency_weight=0.1,
        smoothness_weight=0.1,
    )
    
    spatial_weights = torch.sigmoid(torch.randn(2, 1, 14, 14))
    
    loss = spatial_sup(spatial_weights, features)
    print(f"   æ€»æŸå¤±: {loss.total_loss.item():.4f}")
    print(f"   æŸå¤±ç»„ä»¶: {list(loss.components.keys())}")
    
    print("\n3. Transformer æ³¨æ„åŠ›ç›‘ç£:")
    transformer_sup = TransformerAttentionSupervision(
        loss_weight=0.1,
        head_diversity_weight=0.1,
        locality_weight=0.1,
    )
    
    transformer_weights = torch.softmax(torch.randn(2, 8, 196, 196), dim=-1)
    features_seq = torch.randn(2, 196, 256)
    
    loss = transformer_sup(transformer_weights, features_seq)
    print(f"   æ€»æŸå¤±: {loss.total_loss.item():.4f}")
    print(f"   æŸå¤±ç»„ä»¶: {list(loss.components.keys())}")


def demo_integration_example():
    """æ¼”ç¤ºé›†æˆç¤ºä¾‹"""
    print("\n" + "=" * 60)
    print("é›†æˆç¤ºä¾‹")
    print("=" * 60)
    
    print("\nå®Œæ•´çš„æ¨¡å‹é›†æˆç¤ºä¾‹:")
    code = '''
import torch.nn as nn
from med_core.attention_supervision import (
    SEAttention,
    ChannelAttentionSupervision,
)

class AttentionEnhancedModel(nn.Module):
    def __init__(self):
        super().__init__()
        
        # éª¨å¹²ç½‘ç»œ
        self.backbone = nn.Sequential(
            nn.Conv2d(3, 64, 3, padding=1),
            nn.ReLU(),
            nn.Conv2d(64, 256, 3, padding=1),
            nn.ReLU(),
        )
        
        # SE æ³¨æ„åŠ›
        self.attention = SEAttention(channels=256, reduction=16)
        
        # æ³¨æ„åŠ›ç›‘ç£
        self.attention_supervision = ChannelAttentionSupervision(
            loss_weight=0.1,
            diversity_weight=0.1,
        )
        
        # åˆ†ç±»å¤´
        self.classifier = nn.Sequential(
            nn.AdaptiveAvgPool2d(1),
            nn.Flatten(),
            nn.Linear(256, 10),
        )
    
    def forward(self, x, return_attention=False):
        # ç‰¹å¾æå–
        features = self.backbone(x)
        
        # åº”ç”¨æ³¨æ„åŠ›
        attended_features = self.attention(features)
        
        # åˆ†ç±»
        logits = self.classifier(attended_features)
        
        if return_attention:
            # è·å–æ³¨æ„åŠ›æƒé‡
            attn_weights = self.attention.get_attention_weights(features)
            return logits, attn_weights
        
        return logits
    
    def compute_loss(self, x, y):
        # å‰å‘ä¼ æ’­
        logits, attn_weights = self.forward(x, return_attention=True)
        
        # åˆ†ç±»æŸå¤±
        cls_loss = nn.CrossEntropyLoss()(logits, y)
        
        # æ³¨æ„åŠ›ç›‘ç£æŸå¤±
        attn_loss = self.attention_supervision(
            attn_weights,
            self.backbone(x),
        )
        
        # æ€»æŸå¤±
        total_loss = cls_loss + attn_loss.total_loss
        
        return total_loss, {
            "cls_loss": cls_loss.item(),
            "attn_loss": attn_loss.total_loss.item(),
            **{k: v.item() for k, v in attn_loss.components.items()},
        }

# ä½¿ç”¨
model = AttentionEnhancedModel()
x = torch.randn(2, 3, 224, 224)
y = torch.randint(0, 10, (2,))

loss, loss_dict = model.compute_loss(x, y)
print(f"Total loss: {loss.item():.4f}")
print(f"Loss components: {loss_dict}")
'''
    print(code)


def main():
    """ä¸»å‡½æ•°"""
    print("\n" + "=" * 60)
    print("MedFusion é«˜çº§æ³¨æ„åŠ›æ¨¡å—æ¼”ç¤º")
    print("=" * 60)
    
    try:
        # æ¼”ç¤º 1: SE æ³¨æ„åŠ›
        demo_se_attention()
        
        # æ¼”ç¤º 2: ECA æ³¨æ„åŠ›
        demo_eca_attention()
        
        # æ¼”ç¤º 3: ç©ºé—´æ³¨æ„åŠ›
        demo_spatial_attention()
        
        # æ¼”ç¤º 4: CBAM
        demo_cbam()
        
        # æ¼”ç¤º 5: Transformer æ³¨æ„åŠ›
        demo_transformer_attention()
        
        # æ¼”ç¤º 6: å·¥å‚å‡½æ•°
        demo_factory_function()
        
        # æ¼”ç¤º 7: æ³¨æ„åŠ›ç›‘ç£
        demo_attention_supervision()
        
        # æ¼”ç¤º 8: é›†æˆç¤ºä¾‹
        demo_integration_example()
        
        print("\n" + "=" * 60)
        print("æ¼”ç¤ºå®Œæˆï¼")
        print("=" * 60)
        
        print("\nğŸ’¡ å…³é”®è¦ç‚¹:")
        print("  1. SE/ECA é€‚åˆé€šé“æ³¨æ„åŠ›")
        print("  2. ç©ºé—´æ³¨æ„åŠ›é€‚åˆç›®æ ‡å®šä½")
        print("  3. CBAM ç»“åˆä¸¤è€…ä¼˜åŠ¿")
        print("  4. Transformer é€‚åˆå…¨å±€å»ºæ¨¡")
        print("  5. æ³¨æ„åŠ›ç›‘ç£æé«˜å¯è§£é‡Šæ€§")
        
        print("\nğŸ“– ç›¸å…³èµ„æº:")
        print("  â€¢ med_core/attention_supervision/advanced_attention.py")
        print("  â€¢ med_core/attention_supervision/advanced_supervision.py")
        print("  â€¢ examples/advanced_attention_demo.py")
        
    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
