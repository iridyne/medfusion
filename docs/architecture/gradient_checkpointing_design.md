# æ¢¯åº¦æ£€æŸ¥ç‚¹æ¶æ„è®¾è®¡æ–‡æ¡£

## ğŸ“‹ æ¦‚è¿°

æœ¬æ–‡æ¡£æè¿° MedFusion æ¡†æ¶ä¸­æ¢¯åº¦æ£€æŸ¥ç‚¹åŠŸèƒ½çš„æ¶æ„è®¾è®¡ï¼Œæ—¨åœ¨ä¸ºæœªæ¥æ‰©å±•æä¾›æ¸…æ™°çš„æŒ‡å¯¼ã€‚

---

## ğŸ—ï¸ æ¶æ„è®¾è®¡

### 1. æ ¸å¿ƒç»„ä»¶

```
med_core/
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ gradient_checkpointing.py    # é€šç”¨å·¥å…·å‡½æ•°
â””â”€â”€ backbones/
    â”œâ”€â”€ base.py                       # åŸºç±»æ¥å£å®šä¹‰
    â”œâ”€â”€ vision.py                     # ResNet å®ç°
    â”œâ”€â”€ swin_2d.py                    # Swin 2D å®ç°
    â”œâ”€â”€ swin_3d.py                    # Swin 3D å®ç°
    â”œâ”€â”€ efficientnet.py               # å¾…å®ç°
    â”œâ”€â”€ convnext.py                   # å¾…å®ç°
    â””â”€â”€ vit.py                        # å¾…å®ç°
```

### 2. è®¾è®¡åŸåˆ™

#### 2.1 åˆ†å±‚è®¾è®¡

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   BaseVisionBackbone (åŸºç±»)         â”‚
â”‚   - enable_gradient_checkpointing() â”‚
â”‚   - disable_gradient_checkpointing()â”‚
â”‚   - is_gradient_checkpointing_enabled()â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â–²
              â”‚ ç»§æ‰¿
              â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                            â”‚
â”‚  å…·ä½“ Backbone å®ç°        â”‚
â”‚  - é‡å†™ enable_gradient_   â”‚
â”‚    checkpointing()         â”‚
â”‚  - å®ç°æ¨¡å‹ç‰¹å®šçš„æ£€æŸ¥ç‚¹é€»è¾‘â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 2.2 æ¥å£ç»Ÿä¸€

æ‰€æœ‰ backbone å¿…é¡»å®ç°ä»¥ä¸‹æ¥å£ï¼š

```python
class BaseVisionBackbone(nn.Module):
    def enable_gradient_checkpointing(self, segments: int | None = None) -> None:
        """å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹"""
        
    def disable_gradient_checkpointing(self) -> None:
        """ç¦ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹"""
        
    def is_gradient_checkpointing_enabled(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦å¯ç”¨"""
```

#### 2.3 å·¥å…·å‡½æ•°å¤ç”¨

```python
# med_core/utils/gradient_checkpointing.py
def checkpoint_sequential(functions, segments, input, **kwargs):
    """å¯¹é¡ºåºæ¨¡å—åº”ç”¨æ£€æŸ¥ç‚¹"""
    
def apply_gradient_checkpointing(model, segments):
    """è‡ªåŠ¨åº”ç”¨æ£€æŸ¥ç‚¹åˆ°æ¨¡å‹"""
    
def estimate_memory_savings(model, input_shape, device):
    """ä¼°ç®—å†…å­˜èŠ‚çœ"""
```

---

## ğŸ”§ å®ç°æ¨¡å¼

### æ¨¡å¼ 1: é¡ºåºå±‚æ¨¡å‹ï¼ˆResNet, EfficientNet, MobileNetï¼‰

**ç‰¹ç‚¹**: æ¨¡å‹ç”±é¡ºåºçš„å±‚ç»„æˆ

**å®ç°ç­–ç•¥**:
1. æ•è·åŸå§‹å±‚åˆ—è¡¨
2. å°†å±‚åˆ†æ®µ
3. å¯¹æ¯æ®µåº”ç”¨ `checkpoint_sequential`

**ä»£ç æ¨¡æ¿**:
```python
def enable_gradient_checkpointing(self, segments: int | None = None) -> None:
    super().enable_gradient_checkpointing(segments)
    
    if segments is None:
        segments = 4  # é»˜è®¤æ®µæ•°
    
    from med_core.utils.gradient_checkpointing import checkpoint_sequential
    
    # æ•è·åŸå§‹å±‚
    original_layers = list(self._backbone.children())
    
    def checkpointed_forward(x: torch.Tensor) -> torch.Tensor:
        if not self.training or not self._gradient_checkpointing_enabled:
            # æ­£å¸¸å‰å‘ä¼ æ’­
            for layer in original_layers:
                x = layer(x)
            return x
        
        # ä½¿ç”¨æ£€æŸ¥ç‚¹
        x = checkpoint_sequential(
            original_layers,
            segments=segments,
            input=x,
            use_reentrant=False,
        )
        return x
    
    # æ›¿æ¢ forward æ–¹æ³•
    self._backbone.forward = checkpointed_forward
```

**é€‚ç”¨æ¨¡å‹**:
- âœ… ResNet (å·²å®ç°)
- â³ EfficientNet (å¾…å®ç°)
- â³ MobileNet (å¾…å®ç°)
- â³ DenseNet (å¾…å®ç°)

---

### æ¨¡å¼ 2: Transformer æ¨¡å‹ï¼ˆSwin, ViTï¼‰

**ç‰¹ç‚¹**: æ¨¡å‹ç”±å¤šä¸ª stage/block ç»„æˆï¼Œæ¯ä¸ª stage åŒ…å«å¤šä¸ª transformer block

**å®ç°ç­–ç•¥**:
1. æ•è· patch embeddingã€position encodingã€transformer stagesã€normalization
2. å¯¹ transformer stages åº”ç”¨æ£€æŸ¥ç‚¹
3. ä¿æŒå…¶ä»–ç»„ä»¶æ­£å¸¸è¿è¡Œ

**ä»£ç æ¨¡æ¿**:
```python
def enable_gradient_checkpointing(self, segments: int | None = None) -> None:
    super().enable_gradient_checkpointing(segments)
    
    if segments is None:
        segments = len(self._backbone.layers)  # é»˜è®¤æ¯ä¸ª stage ä¸€ä¸ªæ®µ
    
    from med_core.utils.gradient_checkpointing import checkpoint_sequential
    
    # æ•è·åŸå§‹ç»„ä»¶
    patch_embed = self._backbone.patch_embed
    pos_drop = self._backbone.pos_drop
    layers = list(self._backbone.layers)
    norm = self._backbone.norm
    
    def checkpointed_forward(x: torch.Tensor, normalize: bool = True) -> torch.Tensor:
        if not self.training or not self._gradient_checkpointing_enabled:
            # æ­£å¸¸å‰å‘ä¼ æ’­
            x = patch_embed(x)
            x = pos_drop(x)
            for layer in layers:
                x = layer(x)
            if normalize:
                x = norm(x)
            return x
        
        # Patch embedding (ä¸ä½¿ç”¨æ£€æŸ¥ç‚¹)
        x = patch_embed(x)
        x = pos_drop(x)
        
        # Transformer stages (ä½¿ç”¨æ£€æŸ¥ç‚¹)
        x = checkpoint_sequential(
            layers,
            segments=min(segments, len(layers)),
            input=x,
            use_reentrant=False,
        )
        
        # Normalization (ä¸ä½¿ç”¨æ£€æŸ¥ç‚¹)
        if normalize:
            x = norm(x)
        
        return x
    
    # æ›¿æ¢ forward æ–¹æ³•
    self._backbone.forward = checkpointed_forward
```

**é€‚ç”¨æ¨¡å‹**:
- âœ… Swin Transformer 2D (å·²å®ç°)
- âœ… Swin Transformer 3D (å·²å®ç°)
- â³ ViT (å¾…å®ç°)
- â³ MaxViT (å¾…å®ç°)

---

### æ¨¡å¼ 3: æ··åˆæ¶æ„ï¼ˆConvNeXtï¼‰

**ç‰¹ç‚¹**: ç»“åˆäº†å·ç§¯å’Œç°ä»£æ¶æ„è®¾è®¡

**å®ç°ç­–ç•¥**:
1. è¯†åˆ«æ¨¡å‹çš„ä¸»è¦ stage
2. å¯¹æ¯ä¸ª stage åº”ç”¨æ£€æŸ¥ç‚¹
3. ä¿æŒ stem å’Œ head æ­£å¸¸è¿è¡Œ

**ä»£ç æ¨¡æ¿**:
```python
def enable_gradient_checkpointing(self, segments: int | None = None) -> None:
    super().enable_gradient_checkpointing(segments)
    
    if segments is None:
        segments = 4  # ConvNeXt é€šå¸¸æœ‰ 4 ä¸ª stage
    
    from med_core.utils.gradient_checkpointing import checkpoint_sequential
    
    # æ•è·åŸå§‹ç»„ä»¶
    stem = self._backbone.stem
    stages = list(self._backbone.stages)
    norm = self._backbone.norm
    
    def checkpointed_forward(x: torch.Tensor) -> torch.Tensor:
        if not self.training or not self._gradient_checkpointing_enabled:
            # æ­£å¸¸å‰å‘ä¼ æ’­
            x = stem(x)
            for stage in stages:
                x = stage(x)
            x = norm(x)
            return x
        
        # Stem (ä¸ä½¿ç”¨æ£€æŸ¥ç‚¹)
        x = stem(x)
        
        # Stages (ä½¿ç”¨æ£€æŸ¥ç‚¹)
        x = checkpoint_sequential(
            stages,
            segments=min(segments, len(stages)),
            input=x,
            use_reentrant=False,
        )
        
        # Normalization (ä¸ä½¿ç”¨æ£€æŸ¥ç‚¹)
        x = norm(x)
        
        return x
    
    # æ›¿æ¢ forward æ–¹æ³•
    self._backbone.forward = checkpointed_forward
```

**é€‚ç”¨æ¨¡å‹**:
- â³ ConvNeXt (å¾…å®ç°)
- â³ ConvNeXt V2 (å¾…å®ç°)

---

## ğŸ“ å®ç°æ¸…å•

### å·²å®Œæˆ âœ…
- [x] æ ¸å¿ƒå·¥å…·æ¨¡å— (`gradient_checkpointing.py`)
- [x] åŸºç±»æ¥å£ (`BaseVisionBackbone`)
- [x] ResNet ç³»åˆ—
- [x] Swin Transformer 2D
- [x] Swin Transformer 3D
- [x] æµ‹è¯•å¥—ä»¶
- [x] ä½¿ç”¨æ–‡æ¡£
- [x] æ¼”ç¤ºè„šæœ¬

### å¾…å®ç° â³

#### é«˜ä¼˜å…ˆçº§
- [ ] EfficientNet ç³»åˆ— (æ¨¡å¼ 1)
- [ ] ConvNeXt ç³»åˆ— (æ¨¡å¼ 3)
- [ ] ViT ç³»åˆ— (æ¨¡å¼ 2)

#### ä¸­ä¼˜å…ˆçº§
- [ ] MobileNet ç³»åˆ— (æ¨¡å¼ 1)
- [ ] MaxViT (æ¨¡å¼ 2)
- [ ] RegNet ç³»åˆ— (æ¨¡å¼ 1)

#### ä½ä¼˜å…ˆçº§
- [ ] DenseNet ç³»åˆ— (æ¨¡å¼ 1)
- [ ] å…¶ä»–è‡ªå®šä¹‰ backbone

---

## ğŸ¯ æ‰©å±•æŒ‡å—

### ä¸ºæ–° Backbone æ·»åŠ æ¢¯åº¦æ£€æŸ¥ç‚¹æ”¯æŒ

#### æ­¥éª¤ 1: åˆ†ææ¨¡å‹ç»“æ„

```python
# æ‰“å°æ¨¡å‹ç»“æ„
model = YourBackbone()
print(model)

# æŸ¥çœ‹å­æ¨¡å—
for name, module in model.named_children():
    print(f"{name}: {type(module)}")
```

#### æ­¥éª¤ 2: ç¡®å®šå®ç°æ¨¡å¼

- é¡ºåºå±‚æ¨¡å‹ â†’ ä½¿ç”¨æ¨¡å¼ 1
- Transformer æ¨¡å‹ â†’ ä½¿ç”¨æ¨¡å¼ 2
- æ··åˆæ¶æ„ â†’ ä½¿ç”¨æ¨¡å¼ 3

#### æ­¥éª¤ 3: å®ç° `enable_gradient_checkpointing`

```python
def enable_gradient_checkpointing(self, segments: int | None = None) -> None:
    """
    ä¸º YourBackbone å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ã€‚
    
    Args:
        segments: æ£€æŸ¥ç‚¹æ®µæ•°ï¼ˆNone = è‡ªåŠ¨é€‰æ‹©ï¼‰
    """
    super().enable_gradient_checkpointing(segments)
    
    # 1. è®¾ç½®é»˜è®¤æ®µæ•°
    if segments is None:
        segments = 4  # æ ¹æ®æ¨¡å‹ç»“æ„è°ƒæ•´
    
    # 2. å¯¼å…¥å·¥å…·å‡½æ•°
    from med_core.utils.gradient_checkpointing import checkpoint_sequential
    
    # 3. æ•è·åŸå§‹ç»„ä»¶
    # ... æ ¹æ®æ¨¡å‹ç»“æ„æ•è·
    
    # 4. å®šä¹‰æ£€æŸ¥ç‚¹ forward
    def checkpointed_forward(x: torch.Tensor) -> torch.Tensor:
        if not self.training or not self._gradient_checkpointing_enabled:
            # æ­£å¸¸å‰å‘ä¼ æ’­
            pass
        else:
            # ä½¿ç”¨æ£€æŸ¥ç‚¹
            pass
    
    # 5. æ›¿æ¢ forward æ–¹æ³•
    self._backbone.forward = checkpointed_forward
```

#### æ­¥éª¤ 4: æ·»åŠ æµ‹è¯•

```python
# tests/test_gradient_checkpointing.py

def test_your_backbone_gradient_checkpointing():
    """æµ‹è¯• YourBackbone çš„æ¢¯åº¦æ£€æŸ¥ç‚¹ã€‚"""
    backbone = YourBackbone(variant="base")
    
    # å¯ç”¨æ£€æŸ¥ç‚¹
    backbone.enable_gradient_checkpointing()
    assert backbone.is_gradient_checkpointing_enabled()
    
    # æµ‹è¯•å‰å‘ä¼ æ’­
    backbone.train()
    x = torch.randn(2, 3, 224, 224, requires_grad=True)
    output = backbone(x)
    
    # æµ‹è¯•åå‘ä¼ æ’­
    loss = output.sum()
    loss.backward()
    assert x.grad is not None
```

#### æ­¥éª¤ 5: æ›´æ–°æ–‡æ¡£

åœ¨ `docs/guides/gradient_checkpointing.md` ä¸­æ·»åŠ ï¼š

```markdown
### YourBackbone

YourBackbone çš„ç‰¹ç‚¹...

```python
from med_core.backbones.your_backbone import YourBackbone

backbone = YourBackbone(variant="base")
backbone.enable_gradient_checkpointing()
```

**å†…å­˜èŠ‚çœ**: ~XX%
```

---

## âš ï¸ å¸¸è§é™·é˜±

### 1. é€’å½’é”™è¯¯

âŒ **é”™è¯¯åšæ³•**:
```python
original_forward = self._backbone.forward
def new_forward(x):
    return original_forward(x)  # ä¼šé€’å½’è°ƒç”¨è‡ªå·±ï¼
```

âœ… **æ­£ç¡®åšæ³•**:
```python
original_layers = list(self._backbone.children())
def new_forward(x):
    for layer in original_layers:
        x = layer(x)
    return x
```

### 2. å¿˜è®°æ£€æŸ¥è®­ç»ƒæ¨¡å¼

âŒ **é”™è¯¯åšæ³•**:
```python
def checkpointed_forward(x):
    # æ€»æ˜¯ä½¿ç”¨æ£€æŸ¥ç‚¹ï¼Œæ¨ç†æ—¶ä¹Ÿä¼šå˜æ…¢
    return checkpoint_sequential(layers, segments, x)
```

âœ… **æ­£ç¡®åšæ³•**:
```python
def checkpointed_forward(x):
    if not self.training or not self._gradient_checkpointing_enabled:
        # æ¨ç†æ—¶ä¸ä½¿ç”¨æ£€æŸ¥ç‚¹
        return normal_forward(x)
    return checkpoint_sequential(layers, segments, x)
```

### 3. æ®µæ•°è®¾ç½®ä¸å½“

âŒ **é”™è¯¯åšæ³•**:
```python
segments = 100  # å¤ªå¤šæ®µï¼Œå¼€é”€å¤§äºæ”¶ç›Š
```

âœ… **æ­£ç¡®åšæ³•**:
```python
# æ ¹æ®æ¨¡å‹ç»“æ„é€‰æ‹©åˆç†çš„æ®µæ•°
if segments is None:
    segments = len(self._backbone.stages)  # é€šå¸¸ 4-8 æ®µ
```

---

## ğŸ“Š æ€§èƒ½æŒ‡æ ‡

### å†…å­˜èŠ‚çœ vs è®­ç»ƒæ—¶é—´

| æ®µæ•° | å†…å­˜èŠ‚çœ | è®­ç»ƒæ—¶é—´å¢åŠ  | æ¨èåœºæ™¯ |
|------|----------|--------------|----------|
| 2    | ~20%     | ~10%         | æ˜¾å­˜å……è¶³ï¼Œè¿½æ±‚é€Ÿåº¦ |
| 4    | ~35%     | ~20%         | å¹³è¡¡ï¼ˆæ¨èï¼‰ |
| 8    | ~45%     | ~30%         | æ˜¾å­˜æåº¦å—é™ |
| 16   | ~50%     | ~40%         | ä¸æ¨èï¼ˆæ”¶ç›Šé€’å‡ï¼‰ |

### ä¸åŒæ¨¡å‹çš„æœ€ä½³æ®µæ•°

| æ¨¡å‹ | æ¨èæ®µæ•° | åŸå›  |
|------|----------|------|
| ResNet18/34 | 4 | 4 ä¸ªä¸»è¦ stage |
| ResNet50/101 | 4 | 4 ä¸ªä¸»è¦ stage |
| Swin-Tiny | 4 | 4 ä¸ª transformer stage |
| Swin-Base | 4-8 | æ›´æ·±çš„ç½‘ç»œå¯ä»¥ç”¨æ›´å¤šæ®µ |
| ViT-Base | 6-12 | 12 ä¸ª transformer block |
| EfficientNet | 4-7 | æ ¹æ®å˜ä½“è°ƒæ•´ |

---

## ğŸ”„ ç‰ˆæœ¬å†å²

### v0.2.0 (2024-02)
- âœ… åˆå§‹å®ç°
- âœ… æ”¯æŒ ResNet ç³»åˆ—
- âœ… æ”¯æŒ Swin Transformer 2D/3D
- âœ… å®Œæ•´çš„æµ‹è¯•å’Œæ–‡æ¡£

### v0.3.0 (è®¡åˆ’ä¸­)
- â³ æ”¯æŒ EfficientNet
- â³ æ”¯æŒ ConvNeXt
- â³ æ”¯æŒ ViT
- â³ æ”¯æŒ MobileNet

---

## ğŸ“š å‚è€ƒèµ„æ–™

- [PyTorch Gradient Checkpointing](https://pytorch.org/docs/stable/checkpoint.html)
- [Training Deep Nets with Sublinear Memory Cost](https://arxiv.org/abs/1604.06174)
- [ä½¿ç”¨æŒ‡å—](gradient_checkpointing.md)
- [API æ–‡æ¡£](../api/utils.md#gradient-checkpointing)

---

**ç»´æŠ¤è€…**: MedFusion Team  
**æœ€åæ›´æ–°**: 2024-02-20
