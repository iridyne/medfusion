# æ€§èƒ½åŸºå‡†æµ‹è¯•

## æ¦‚è¿°

æ€§èƒ½åŸºå‡†æµ‹è¯•æ˜¯ç¡®ä¿ä»£ç è´¨é‡çš„é‡è¦ç¯èŠ‚ã€‚MedFusion æä¾›äº†å®Œæ•´çš„åŸºå‡†æµ‹è¯•å·¥å…·ï¼Œç”¨äºï¼š

- æµ‹é‡å…³é”®ç»„ä»¶çš„æ€§èƒ½
- å»ºç«‹æ€§èƒ½åŸºçº¿
- æ£€æµ‹æ€§èƒ½å›å½’
- è¿½è¸ªæ€§èƒ½æ”¹è¿›

## å¿«é€Ÿå¼€å§‹

### è¿è¡ŒåŸºå‡†æµ‹è¯•

```bash
# è¿è¡Œæ‰€æœ‰åŸºå‡†æµ‹è¯•
python scripts/run_benchmarks.py

# è¿è¡Œç‰¹å®šæµ‹è¯•
python scripts/run_benchmarks.py --tests data fusion

# æŒ‡å®šè¾“å‡ºæ–‡ä»¶
python scripts/run_benchmarks.py --output benchmarks/v0.2.0.json
```

### æ¯”è¾ƒç»“æœ

```bash
# ä¸åŸºçº¿æ¯”è¾ƒ
python scripts/compare_benchmarks.py \
    --baseline benchmarks/baseline.json \
    --current benchmarks/current.json

# åœ¨ CI ä¸­ä½¿ç”¨ï¼ˆæœ‰å›å½’æ—¶å¤±è´¥ï¼‰
python scripts/compare_benchmarks.py \
    --baseline benchmarks/baseline.json \
    --current benchmarks/current.json \
    --fail-on-regression
```

## åŸºå‡†æµ‹è¯•å·¥å…·

### 1. PerformanceBenchmark

é€šç”¨çš„æ€§èƒ½åŸºå‡†æµ‹è¯•å™¨ã€‚

**ç‰¹ç‚¹**:
- è‡ªåŠ¨é¢„çƒ­
- å¤šæ¬¡è¿­ä»£
- å†…å­˜è·Ÿè¸ª
- GPU æ”¯æŒ

**ç¤ºä¾‹**:
```python
from med_core.utils.benchmark import PerformanceBenchmark

# åˆ›å»ºåŸºå‡†æµ‹è¯•
benchmark = PerformanceBenchmark(
    name="my_function",
    warmup_iterations=10,
    test_iterations=100,
    device="cuda",
)

# è¿è¡Œæµ‹è¯•
result = benchmark.run(my_function, arg1, arg2)

# æŸ¥çœ‹ç»“æœ
print(result)
# Output:
# my_function:
#   Duration: 0.123s
#   Throughput: 813.0 samples/s
#   Memory: 256.0MB allocated, 512.0MB reserved
```

### 2. DataLoaderBenchmark

æ•°æ®åŠ è½½æ€§èƒ½æµ‹è¯•ã€‚

**ç¤ºä¾‹**:
```python
from med_core.utils.benchmark import DataLoaderBenchmark

benchmark = DataLoaderBenchmark(
    dataset=my_dataset,
    batch_size=32,
    num_workers=4,
)

result = benchmark.run(num_batches=100)
print(f"ååé‡: {result.throughput:.1f} samples/s")
```

### 3. ModelBenchmark

æ¨¡å‹æ¨ç†æ€§èƒ½æµ‹è¯•ã€‚

**ç¤ºä¾‹**:
```python
from med_core.utils.benchmark import ModelBenchmark

benchmark = ModelBenchmark(
    model=my_model,
    input_shape=(3, 224, 224),
    device="cuda",
)

result = benchmark.run(batch_size=32, num_iterations=100)
print(f"ååé‡: {result.throughput:.1f} samples/s")
```

### 4. BenchmarkSuite

ç®¡ç†å¤šä¸ªåŸºå‡†æµ‹è¯•ã€‚

**ç¤ºä¾‹**:
```python
from med_core.utils.benchmark import BenchmarkSuite

# åˆ›å»ºå¥—ä»¶
suite = BenchmarkSuite(name="v0.2.0")

# æ·»åŠ æµ‹è¯•
suite.add_benchmark("test1", lambda: test_function1())
suite.add_benchmark("test2", lambda: test_function2())

# è¿è¡Œæ‰€æœ‰æµ‹è¯•
results = suite.run_all()

# ä¿å­˜ç»“æœ
suite.save_results("baseline.json")

# ä¸åŸºçº¿æ¯”è¾ƒ
suite.compare_with("previous_baseline.json")
```

## æµ‹è¯•ç±»å‹

### 1. æ•°æ®åŠ è½½æµ‹è¯•

æµ‹è¯•æ•°æ®åŠ è½½çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬ï¼š
- æ— ç¼“å­˜åŠ è½½
- æœ‰ç¼“å­˜åŠ è½½
- ä¸åŒ num_workers çš„å½±å“

**å½“å‰åŸºçº¿** (v0.2.0):
- æ— ç¼“å­˜: ~900 samples/s
- æœ‰ç¼“å­˜: ~9,000 samples/s
- åŠ é€Ÿæ¯”: 10x

### 2. èåˆç­–ç•¥æµ‹è¯•

æµ‹è¯•ä¸åŒèåˆç­–ç•¥çš„æ€§èƒ½ï¼š
- Concatenate: ~22M ops/s
- Gated: ~10M ops/s
- Attention: ~8M ops/s

### 3. èšåˆå™¨æµ‹è¯•

æµ‹è¯•ä¸åŒèšåˆæ–¹æ³•çš„æ€§èƒ½ï¼š
- Mean Pooling: ~100K ops/s
- Max Pooling: ~127K ops/s
- Attention Pooling: ~18K ops/s

### 4. é¢„å¤„ç†æµ‹è¯•

æµ‹è¯•å›¾åƒé¢„å¤„ç†æ“ä½œï¼š
- Resize: ~6,700 ops/s
- Normalize: ~950 ops/s
- Augment: ~9,100 ops/s

## æ€§èƒ½å›å½’æ£€æµ‹

### ä»€ä¹ˆæ˜¯æ€§èƒ½å›å½’ï¼Ÿ

æ€§èƒ½å›å½’æ˜¯æŒ‡ä»£ç å˜æ›´å¯¼è‡´æ€§èƒ½ä¸‹é™è¶…è¿‡å¯æ¥å—çš„é˜ˆå€¼ï¼ˆé»˜è®¤ 5%ï¼‰ã€‚

### æ£€æµ‹æµç¨‹

1. **å»ºç«‹åŸºçº¿**
   ```bash
   python scripts/run_benchmarks.py --output benchmarks/baseline.json
   ```

2. **ä¿®æ”¹ä»£ç **
   ```bash
   # è¿›è¡Œä»£ç ä¿®æ”¹
   git commit -m "Optimize data loading"
   ```

3. **è¿è¡Œæ–°æµ‹è¯•**
   ```bash
   python scripts/run_benchmarks.py --output benchmarks/current.json
   ```

4. **æ¯”è¾ƒç»“æœ**
   ```bash
   python scripts/compare_benchmarks.py \
       --baseline benchmarks/baseline.json \
       --current benchmarks/current.json
   ```

### è§£è¯»ç»“æœ

```
âŒ æ€§èƒ½å›å½’ (ä¸‹é™ > 5%):
  data_loading.throughput:
    åŸºçº¿: 1000.0
    å½“å‰: 900.0
    å˜åŒ–: -10.0%

âœ… æ€§èƒ½æ”¹è¿› (æå‡ > 5%):
  fusion.throughput:
    åŸºçº¿: 1000.0
    å½“å‰: 1200.0
    å˜åŒ–: +20.0%

æ€»ç»“:
  å›å½’: 1
  æ”¹è¿›: 1
  ç¨³å®š: 20
```

## CI/CD é›†æˆ

### GitHub Actions

åœ¨ `.github/workflows/benchmark.yml` ä¸­æ·»åŠ ï¼š

```yaml
name: Performance Benchmarks

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]

jobs:
  benchmark:
    runs-on: ubuntu-latest
    
    steps:
      - uses: actions/checkout@v3
      
      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
      
      - name: Install dependencies
        run: |
          pip install -e .
      
      - name: Run benchmarks
        run: |
          python scripts/run_benchmarks.py \
            --output benchmarks/current.json
      
      - name: Download baseline
        uses: actions/download-artifact@v3
        with:
          name: benchmark-baseline
          path: benchmarks/
        continue-on-error: true
      
      - name: Compare with baseline
        if: success()
        run: |
          if [ -f benchmarks/baseline.json ]; then
            python scripts/compare_benchmarks.py \
              --baseline benchmarks/baseline.json \
              --current benchmarks/current.json \
              --fail-on-regression
          else
            echo "No baseline found, skipping comparison"
          fi
      
      - name: Upload results
        uses: actions/upload-artifact@v3
        with:
          name: benchmark-results
          path: benchmarks/current.json
      
      - name: Update baseline (on main)
        if: github.ref == 'refs/heads/main'
        uses: actions/upload-artifact@v3
        with:
          name: benchmark-baseline
          path: benchmarks/current.json
```

### GitLab CI

åœ¨ `.gitlab-ci.yml` ä¸­æ·»åŠ ï¼š

```yaml
benchmark:
  stage: test
  script:
    - pip install -e .
    - python scripts/run_benchmarks.py --output benchmarks/current.json
    - |
      if [ -f benchmarks/baseline.json ]; then
        python scripts/compare_benchmarks.py \
          --baseline benchmarks/baseline.json \
          --current benchmarks/current.json \
          --fail-on-regression
      fi
  artifacts:
    paths:
      - benchmarks/
    expire_in: 30 days
```

## æ€§èƒ½ä¼˜åŒ–å·¥ä½œæµ

### 1. è¯†åˆ«ç“¶é¢ˆ

```python
# ä½¿ç”¨ profiler æ‰¾åˆ°æ…¢çš„éƒ¨åˆ†
import cProfile
import pstats

profiler = cProfile.Profile()
profiler.enable()

# è¿è¡Œä»£ç 
train_one_epoch()

profiler.disable()
stats = pstats.Stats(profiler)
stats.sort_stats('cumulative')
stats.print_stats(20)
```

### 2. å®æ–½ä¼˜åŒ–

```python
# ä¼˜åŒ–å‰
def slow_function():
    result = []
    for i in range(1000):
        result.append(expensive_operation(i))
    return result

# ä¼˜åŒ–å
def fast_function():
    # ä½¿ç”¨ç¼“å­˜
    cache = {}
    result = []
    for i in range(1000):
        if i not in cache:
            cache[i] = expensive_operation(i)
        result.append(cache[i])
    return result
```

### 3. éªŒè¯æ”¹è¿›

```python
from med_core.utils.benchmark import PerformanceBenchmark

# æµ‹è¯•ä¼˜åŒ–å‰
benchmark_slow = PerformanceBenchmark("slow")
result_slow = benchmark_slow.run(slow_function)

# æµ‹è¯•ä¼˜åŒ–å
benchmark_fast = PerformanceBenchmark("fast")
result_fast = benchmark_fast.run(fast_function)

# è®¡ç®—åŠ é€Ÿæ¯”
speedup = result_fast.throughput / result_slow.throughput
print(f"åŠ é€Ÿæ¯”: {speedup:.1f}x")
```

### 4. æ›´æ–°åŸºçº¿

```bash
# å¦‚æœä¼˜åŒ–æ•ˆæœæ»¡æ„ï¼Œæ›´æ–°åŸºçº¿
cp benchmarks/current.json benchmarks/baseline.json
git add benchmarks/baseline.json
git commit -m "Update performance baseline after optimization"
```

## æœ€ä½³å®è·µ

### âœ… æ¨èåšæ³•

1. **å®šæœŸè¿è¡ŒåŸºå‡†æµ‹è¯•**
   - æ¯æ¬¡é‡è¦ä»£ç å˜æ›´å
   - æ¯å‘¨/æ¯æœˆå®šæœŸæµ‹è¯•
   - å‘å¸ƒå‰å¿…é¡»æµ‹è¯•

2. **ä½¿ç”¨å›ºå®šçš„æµ‹è¯•ç¯å¢ƒ**
   - ç›¸åŒçš„ç¡¬ä»¶
   - ç›¸åŒçš„è½¯ä»¶ç‰ˆæœ¬
   - éš”ç¦»çš„æµ‹è¯•ç¯å¢ƒ

3. **è®°å½•ç¯å¢ƒä¿¡æ¯**
   ```python
   import platform
   import torch
   
   env_info = {
       "python": platform.python_version(),
       "torch": torch.__version__,
       "cuda": torch.version.cuda,
       "cpu": platform.processor(),
   }
   ```

4. **è®¾ç½®åˆç†çš„é˜ˆå€¼**
   - 5% é€‚åˆå¤§å¤šæ•°æƒ…å†µ
   - å…³é”®è·¯å¾„å¯ä»¥è®¾ç½®æ›´ä¸¥æ ¼ï¼ˆ2-3%ï¼‰
   - éå…³é”®è·¯å¾„å¯ä»¥æ”¾å®½ï¼ˆ10%ï¼‰

5. **è‡ªåŠ¨åŒ–æµ‹è¯•**
   - é›†æˆåˆ° CI/CD
   - è‡ªåŠ¨ç”ŸæˆæŠ¥å‘Š
   - è‡ªåŠ¨é€šçŸ¥å›å½’

### âŒ é¿å…çš„åšæ³•

1. **ä¸è¦åœ¨ä¸ç¨³å®šçš„ç¯å¢ƒæµ‹è¯•**
   - é¿å…åœ¨ç¬”è®°æœ¬ç”µè„‘ä¸Šæµ‹è¯•
   - é¿å…åœ¨å…±äº«æœåŠ¡å™¨ä¸Šæµ‹è¯•
   - é¿å…åœ¨è´Ÿè½½é«˜çš„æ—¶å€™æµ‹è¯•

2. **ä¸è¦å¿½ç•¥é¢„çƒ­**
   - ç¬¬ä¸€æ¬¡è¿è¡Œé€šå¸¸è¾ƒæ…¢
   - éœ€è¦é¢„çƒ­ JITã€ç¼“å­˜ç­‰

3. **ä¸è¦å•æ¬¡æµ‹é‡**
   - å•æ¬¡æµ‹é‡è¯¯å·®å¤§
   - è‡³å°‘è¿è¡Œ 10-100 æ¬¡

4. **ä¸è¦æ¯”è¾ƒä¸åŒç¯å¢ƒçš„ç»“æœ**
   - CPU vs GPU
   - ä¸åŒçš„ Python ç‰ˆæœ¬
   - ä¸åŒçš„åº“ç‰ˆæœ¬

## æ•…éšœæ’é™¤

### é—®é¢˜ 1: ç»“æœä¸ç¨³å®š

**ç—‡çŠ¶**: æ¯æ¬¡è¿è¡Œç»“æœå·®å¼‚å¾ˆå¤§

**åŸå› **:
- ç¯å¢ƒä¸ç¨³å®š
- è¿­ä»£æ¬¡æ•°å¤ªå°‘
- æ²¡æœ‰é¢„çƒ­

**è§£å†³æ–¹æ¡ˆ**:
```python
# å¢åŠ é¢„çƒ­å’Œè¿­ä»£æ¬¡æ•°
benchmark = PerformanceBenchmark(
    name="test",
    warmup_iterations=50,  # å¢åŠ é¢„çƒ­
    test_iterations=500,   # å¢åŠ è¿­ä»£
)

# å›ºå®šéšæœºç§å­
import random
import numpy as np
import torch

random.seed(42)
np.random.seed(42)
torch.manual_seed(42)
```

### é—®é¢˜ 2: GPU å†…å­˜ä¸è¶³

**ç—‡çŠ¶**: CUDA out of memory

**åŸå› **:
- æ‰¹æ¬¡å¤ªå¤§
- æ¨¡å‹å¤ªå¤§
- å†…å­˜æ³„æ¼

**è§£å†³æ–¹æ¡ˆ**:
```python
# å‡å°æ‰¹æ¬¡
benchmark.run(batch_size=16)  # ä» 32 å‡åˆ° 16

# æ¸…ç†ç¼“å­˜
torch.cuda.empty_cache()

# ä½¿ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
model.gradient_checkpointing_enable()
```

### é—®é¢˜ 3: æ¯”è¾ƒå¤±è´¥

**ç—‡çŠ¶**: æ— æ³•æ‰¾åˆ°åŸºçº¿æ–‡ä»¶

**åŸå› **:
- åŸºçº¿æ–‡ä»¶ä¸å­˜åœ¨
- è·¯å¾„é”™è¯¯

**è§£å†³æ–¹æ¡ˆ**:
```bash
# æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
ls -la benchmarks/

# åˆ›å»ºåŸºçº¿
python scripts/run_benchmarks.py --output benchmarks/baseline.json

# ä½¿ç”¨æ­£ç¡®çš„è·¯å¾„
python scripts/compare_benchmarks.py \
    --baseline benchmarks/baseline.json \
    --current benchmarks/current.json
```

## æ€§èƒ½ç›®æ ‡

### æ•°æ®åŠ è½½

| ç»„ä»¶ | ç›®æ ‡ | å½“å‰ | çŠ¶æ€ |
|------|------|------|------|
| æ— ç¼“å­˜åŠ è½½ | > 500 samples/s | 927 samples/s | âœ… |
| æœ‰ç¼“å­˜åŠ è½½ | > 5000 samples/s | 9451 samples/s | âœ… |
| å¤šè¿›ç¨‹åŠ è½½ | > 2000 samples/s | TBD | ğŸ”„ |

### æ¨¡å‹æ¨ç†

| æ¨¡å‹ | è®¾å¤‡ | ç›®æ ‡ | å½“å‰ | çŠ¶æ€ |
|------|------|------|------|------|
| ResNet50 | CPU | > 50 samples/s | TBD | ğŸ”„ |
| ResNet50 | GPU | > 500 samples/s | TBD | ğŸ”„ |
| ViT-B/16 | GPU | > 200 samples/s | TBD | ğŸ”„ |

### èåˆç­–ç•¥

| ç­–ç•¥ | ç›®æ ‡ | å½“å‰ | çŠ¶æ€ |
|------|------|------|------|
| Concatenate | > 10M ops/s | 22M ops/s | âœ… |
| Gated | > 5M ops/s | 10M ops/s | âœ… |
| Attention | > 5M ops/s | 8M ops/s | âœ… |

## API å‚è€ƒ

### PerformanceBenchmark

```python
PerformanceBenchmark(
    name: str,
    warmup_iterations: int = 10,
    test_iterations: int = 100,
    device: str = "cpu",
)
```

**æ–¹æ³•**:
- `run(func, *args, **kwargs)`: è¿è¡ŒåŸºå‡†æµ‹è¯•

### BenchmarkResult

```python
@dataclass
class BenchmarkResult:
    name: str
    duration: float
    throughput: float
    memory_allocated: float
    memory_reserved: float
    metadata: dict
```

**æ–¹æ³•**:
- `to_dict()`: è½¬æ¢ä¸ºå­—å…¸
- `__str__()`: å­—ç¬¦ä¸²è¡¨ç¤º

### BenchmarkSuite

```python
BenchmarkSuite(
    name: str,
    output_dir: str | Path = "./benchmarks",
)
```

**æ–¹æ³•**:
- `add_benchmark(name, func)`: æ·»åŠ æµ‹è¯•
- `run_all()`: è¿è¡Œæ‰€æœ‰æµ‹è¯•
- `save_results(filename)`: ä¿å­˜ç»“æœ
- `compare_with(baseline_file)`: ä¸åŸºçº¿æ¯”è¾ƒ

## ç›¸å…³èµ„æº

- **å®ç°ä»£ç **: `med_core/utils/benchmark.py`
- **è¿è¡Œè„šæœ¬**: `scripts/run_benchmarks.py`
- **æ¯”è¾ƒè„šæœ¬**: `scripts/compare_benchmarks.py`
- **æ¼”ç¤ºè„šæœ¬**: `examples/benchmark_demo.py`
- **åŸºçº¿æ•°æ®**: `benchmarks/baseline.json`

## æ›´æ–°æ—¥å¿—

### v0.2.0 (2026-02-20)
- âœ¨ æ–°å¢æ€§èƒ½åŸºå‡†æµ‹è¯•å·¥å…·
- âœ¨ æ–°å¢å›å½’æ£€æµ‹è„šæœ¬
- âœ¨ å»ºç«‹æ€§èƒ½åŸºçº¿
- ğŸ“ å®Œæ•´çš„æ–‡æ¡£å’Œç¤ºä¾‹
- ğŸ”§ CI/CD é›†æˆæŒ‡å—
