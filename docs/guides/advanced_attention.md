# é«˜çº§æ³¨æ„åŠ›æœºåˆ¶æŒ‡å—

æœ¬æŒ‡å—ä»‹ç» MedFusion v0.2.0 ä¸­æ–°å¢çš„é«˜çº§æ³¨æ„åŠ›æœºåˆ¶ï¼ŒåŒ…æ‹¬ SEã€ECAã€Transformer ç­‰ã€‚

## æ¦‚è¿°

MedFusion ç°åœ¨æ”¯æŒå¤šç§å…ˆè¿›çš„æ³¨æ„åŠ›æœºåˆ¶ï¼š

1. **SE (Squeeze-and-Excitation)**: é€šé“æ³¨æ„åŠ›
2. **ECA (Efficient Channel Attention)**: é«˜æ•ˆé€šé“æ³¨æ„åŠ›
3. **Spatial Attention**: ç©ºé—´æ³¨æ„åŠ›
4. **CBAM**: é€šé“ + ç©ºé—´æ³¨æ„åŠ›
5. **Transformer Attention**: å¤šå¤´è‡ªæ³¨æ„åŠ›

## å¿«é€Ÿå¼€å§‹

### åŸºæœ¬ä½¿ç”¨

```python
from med_core.attention_supervision import create_attention_module

# åˆ›å»º SE æ³¨æ„åŠ›
se_attention = create_attention_module(
    attention_type="se",
    channels=256,
    reduction=16,
)

# åœ¨æ¨¡å‹ä¸­ä½¿ç”¨
import torch
x = torch.randn(2, 256, 14, 14)
out = se_attention(x)  # (2, 256, 14, 14)
```

### å·¥å‚å‡½æ•°

ä½¿ç”¨å·¥å‚å‡½æ•°å¯ä»¥è½»æ¾åˆ‡æ¢ä¸åŒçš„æ³¨æ„åŠ›ç±»å‹ï¼š

```python
from med_core.attention_supervision import create_attention_module

# æ”¯æŒçš„ç±»å‹
attention_types = ["se", "eca", "spatial", "cbam", "transformer"]

for attn_type in attention_types:
    attention = create_attention_module(attn_type, channels=256)
    x = torch.randn(2, 256, 14, 14)
    out = attention(x)
```

## æ³¨æ„åŠ›æ¨¡å—è¯¦è§£

### 1. SE (Squeeze-and-Excitation) æ³¨æ„åŠ›

**åŸç†**: é€šè¿‡å…¨å±€å¹³å‡æ± åŒ–å’Œä¸¤å±‚å…¨è¿æ¥ç½‘ç»œå­¦ä¹ é€šé“æ³¨æ„åŠ›æƒé‡ã€‚

**ä¼˜ç‚¹**:
- å‚æ•°é‡å°
- è®¡ç®—é«˜æ•ˆ
- æ€§èƒ½æå‡æ˜æ˜¾

**ä½¿ç”¨åœºæ™¯**:
- éœ€è¦å¢å¼ºé‡è¦é€šé“
- è®¡ç®—èµ„æºæœ‰é™
- é€šé“æ•°è¾ƒå¤š

**ç¤ºä¾‹**:

```python
from med_core.attention_supervision import SEAttention

# åˆ›å»º SE æ¨¡å—
se = SEAttention(
    channels=256,      # è¾“å…¥é€šé“æ•°
    reduction=16,      # é™ç»´æ¯”ä¾‹
    activation="relu", # æ¿€æ´»å‡½æ•°
)

# å‰å‘ä¼ æ’­
x = torch.randn(2, 256, 14, 14)
out = se(x)  # (2, 256, 14, 14)

# è·å–æ³¨æ„åŠ›æƒé‡ï¼ˆç”¨äºå¯è§†åŒ–ï¼‰
weights = se.get_attention_weights(x)  # (2, 256)
```

**å‚æ•°è¯´æ˜**:
- `channels`: è¾“å…¥é€šé“æ•°
- `reduction`: é™ç»´æ¯”ä¾‹ï¼Œè¶Šå¤§å‚æ•°è¶Šå°‘ï¼ˆé»˜è®¤ 16ï¼‰
- `activation`: æ¿€æ´»å‡½æ•°ï¼Œå¯é€‰ "relu", "gelu", "silu"

**å‚è€ƒæ–‡çŒ®**:
- Hu et al. "Squeeze-and-Excitation Networks" CVPR 2018

---

### 2. ECA (Efficient Channel Attention) æ³¨æ„åŠ›

**åŸç†**: ä½¿ç”¨ 1D å·ç§¯å®ç°é«˜æ•ˆçš„é€šé“æ³¨æ„åŠ›ï¼Œé¿å…é™ç»´ã€‚

**ä¼˜ç‚¹**:
- å‚æ•°é‡æ›´å°‘
- æ€§èƒ½ä¼˜äº SE
- è‡ªé€‚åº”å·ç§¯æ ¸å¤§å°

**ä½¿ç”¨åœºæ™¯**:
- éœ€è¦æè‡´çš„æ•ˆç‡
- é€šé“æ•°å˜åŒ–è¾ƒå¤§
- è¿½æ±‚æœ€ä½³æ€§èƒ½

**ç¤ºä¾‹**:

```python
from med_core.attention_supervision import ECAAttention

# åˆ›å»º ECA æ¨¡å—ï¼ˆè‡ªåŠ¨è®¡ç®—å·ç§¯æ ¸å¤§å°ï¼‰
eca = ECAAttention(channels=256)

# æˆ–æ‰‹åŠ¨æŒ‡å®šå·ç§¯æ ¸å¤§å°
eca = ECAAttention(
    channels=256,
    kernel_size=5,  # æ‰‹åŠ¨æŒ‡å®š
)

# å‰å‘ä¼ æ’­
x = torch.randn(2, 256, 14, 14)
out = eca(x)  # (2, 256, 14, 14)

# è·å–æ³¨æ„åŠ›æƒé‡
weights = eca.get_attention_weights(x)  # (2, 256)
```

**å‚æ•°è¯´æ˜**:
- `channels`: è¾“å…¥é€šé“æ•°
- `kernel_size`: 1D å·ç§¯æ ¸å¤§å°ï¼ˆé»˜è®¤è‡ªåŠ¨è®¡ç®—ï¼‰
- `gamma`, `b`: è‡ªåŠ¨è®¡ç®—å·ç§¯æ ¸å¤§å°çš„å‚æ•°

**å‚è€ƒæ–‡çŒ®**:
- Wang et al. "ECA-Net: Efficient Channel Attention for Deep CNNs" CVPR 2020

---

### 3. Spatial Attention (ç©ºé—´æ³¨æ„åŠ›)

**åŸç†**: å­¦ä¹ ç©ºé—´ç»´åº¦çš„æ³¨æ„åŠ›æƒé‡ï¼Œå…³æ³¨é‡è¦çš„ç©ºé—´ä½ç½®ã€‚

**ä¼˜ç‚¹**:
- é€‚åˆç›®æ ‡å®šä½
- æé«˜ç©ºé—´æ„ŸçŸ¥èƒ½åŠ›
- å¯è§†åŒ–æ•ˆæœå¥½

**ä½¿ç”¨åœºæ™¯**:
- ç›®æ ‡æ£€æµ‹
- ç—…ç¶å®šä½
- æ˜¾è‘—æ€§æ£€æµ‹

**ç¤ºä¾‹**:

```python
from med_core.attention_supervision import SpatialAttention

# åˆ›å»ºç©ºé—´æ³¨æ„åŠ›æ¨¡å—
spatial = SpatialAttention(kernel_size=7)

# å‰å‘ä¼ æ’­
x = torch.randn(2, 256, 14, 14)
out = spatial(x)  # (2, 256, 14, 14)

# è·å–æ³¨æ„åŠ›æƒé‡
weights = spatial.get_attention_weights(x)  # (2, 1, 14, 14)
```

**å‚æ•°è¯´æ˜**:
- `kernel_size`: å·ç§¯æ ¸å¤§å°ï¼ˆé»˜è®¤ 7ï¼‰

---

### 4. CBAM (Convolutional Block Attention Module)

**åŸç†**: ç»“åˆé€šé“æ³¨æ„åŠ›å’Œç©ºé—´æ³¨æ„åŠ›ï¼Œå…ˆé€šé“åç©ºé—´ã€‚

**ä¼˜ç‚¹**:
- æ€§èƒ½å¼ºå¤§
- ç»“åˆä¸¤è€…ä¼˜åŠ¿
- å¹¿æ³›åº”ç”¨

**ä½¿ç”¨åœºæ™¯**:
- éœ€è¦åŒæ—¶å…³æ³¨é€šé“å’Œç©ºé—´
- è¿½æ±‚æœ€ä½³æ€§èƒ½
- é€šç”¨åœºæ™¯

**ç¤ºä¾‹**:

```python
from med_core.attention_supervision import CBAM

# åˆ›å»º CBAM æ¨¡å—
cbam = CBAM(
    channels=256,
    reduction=16,      # é€šé“æ³¨æ„åŠ›çš„é™ç»´æ¯”ä¾‹
    spatial_kernel=7,  # ç©ºé—´æ³¨æ„åŠ›çš„å·ç§¯æ ¸å¤§å°
)

# å‰å‘ä¼ æ’­
x = torch.randn(2, 256, 14, 14)
out = cbam(x)  # (2, 256, 14, 14)
```

**å‚æ•°è¯´æ˜**:
- `channels`: è¾“å…¥é€šé“æ•°
- `reduction`: é€šé“æ³¨æ„åŠ›çš„é™ç»´æ¯”ä¾‹
- `spatial_kernel`: ç©ºé—´æ³¨æ„åŠ›çš„å·ç§¯æ ¸å¤§å°

**å‚è€ƒæ–‡çŒ®**:
- Woo et al. "CBAM: Convolutional Block Attention Module" ECCV 2018

---

### 5. Transformer Attention (Transformer æ³¨æ„åŠ›)

**åŸç†**: å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œå…¨å±€æ„Ÿå—é‡ã€‚

**ä¼˜ç‚¹**:
- å…¨å±€å»ºæ¨¡èƒ½åŠ›
- æ•è·é•¿è·ç¦»ä¾èµ–
- æ€§èƒ½ä¼˜å¼‚

**ä½¿ç”¨åœºæ™¯**:
- éœ€è¦å…¨å±€ä¿¡æ¯
- é•¿è·ç¦»ä¾èµ–å»ºæ¨¡
- å¤§è§„æ¨¡æ•°æ®

**ç¤ºä¾‹**:

```python
from med_core.attention_supervision import TransformerAttention2D

# åˆ›å»º Transformer æ³¨æ„åŠ›æ¨¡å—
transformer = TransformerAttention2D(
    channels=256,
    num_heads=8,
    qkv_bias=False,
    attn_drop=0.0,
    proj_drop=0.0,
)

# å‰å‘ä¼ æ’­
x = torch.randn(2, 256, 14, 14)
out = transformer(x)  # (2, 256, 14, 14)

# è·å–æ³¨æ„åŠ›æƒé‡
weights = transformer.get_attention_weights(x)  # (2, 8, 196, 196)
```

**å‚æ•°è¯´æ˜**:
- `channels`: è¾“å…¥é€šé“æ•°
- `num_heads`: æ³¨æ„åŠ›å¤´æ•°
- `qkv_bias`: æ˜¯å¦ä½¿ç”¨ QKV åç½®
- `attn_drop`: æ³¨æ„åŠ› dropout
- `proj_drop`: æŠ•å½± dropout

---

## æ³¨æ„åŠ›ç›‘ç£

ä¸ºäº†æé«˜æ³¨æ„åŠ›çš„å¯è§£é‡Šæ€§å’Œæ€§èƒ½ï¼ŒMedFusion æä¾›äº†æ³¨æ„åŠ›ç›‘ç£æœºåˆ¶ã€‚

### 1. é€šé“æ³¨æ„åŠ›ç›‘ç£

```python
from med_core.attention_supervision import (
    SEAttention,
    ChannelAttentionSupervision,
)

# åˆ›å»ºæ³¨æ„åŠ›æ¨¡å—å’Œç›‘ç£
se = SEAttention(channels=256, reduction=16)
supervision = ChannelAttentionSupervision(
    loss_weight=0.1,           # æŸå¤±æƒé‡
    diversity_weight=0.1,      # å¤šæ ·æ€§æŸå¤±æƒé‡
    sparsity_weight=0.1,       # ç¨€ç–æ€§æŸå¤±æƒé‡
)

# å‰å‘ä¼ æ’­
x = torch.randn(2, 256, 14, 14)
out = se(x)

# è·å–æ³¨æ„åŠ›æƒé‡
weights = se.get_attention_weights(x)

# è®¡ç®—ç›‘ç£æŸå¤±
loss = supervision(weights, x)
print(f"Total loss: {loss.total_loss.item():.4f}")
print(f"Components: {loss.components}")
```

### 2. ç©ºé—´æ³¨æ„åŠ›ç›‘ç£

```python
from med_core.attention_supervision import (
    SpatialAttention,
    SpatialAttentionSupervision,
)

# åˆ›å»ºæ³¨æ„åŠ›æ¨¡å—å’Œç›‘ç£
spatial = SpatialAttention(kernel_size=7)
supervision = SpatialAttentionSupervision(
    loss_weight=0.1,
    consistency_weight=0.1,    # ä¸€è‡´æ€§æŸå¤±æƒé‡
    smoothness_weight=0.1,     # å¹³æ»‘æ€§æŸå¤±æƒé‡
)

# å‰å‘ä¼ æ’­
x = torch.randn(2, 256, 14, 14)
out = spatial(x)

# è·å–æ³¨æ„åŠ›æƒé‡
weights = spatial.get_attention_weights(x)

# è®¡ç®—ç›‘ç£æŸå¤±ï¼ˆå¯é€‰æä¾›ç›®æ ‡æ©ç ï¼‰
targets = torch.randint(0, 2, (2, 1, 14, 14)).float()
loss = supervision(weights, x, targets)
```

### 3. Transformer æ³¨æ„åŠ›ç›‘ç£

```python
from med_core.attention_supervision import (
    TransformerAttention2D,
    TransformerAttentionSupervision,
)

# åˆ›å»ºæ³¨æ„åŠ›æ¨¡å—å’Œç›‘ç£
transformer = TransformerAttention2D(channels=256, num_heads=8)
supervision = TransformerAttentionSupervision(
    loss_weight=0.1,
    head_diversity_weight=0.1,  # å¤´å¤šæ ·æ€§æŸå¤±æƒé‡
    locality_weight=0.1,         # å±€éƒ¨æ€§æŸå¤±æƒé‡
)

# å‰å‘ä¼ æ’­
x = torch.randn(2, 256, 14, 14)
out = transformer(x)

# è·å–æ³¨æ„åŠ›æƒé‡
weights = transformer.get_attention_weights(x)

# è®¡ç®—ç›‘ç£æŸå¤±
loss = supervision(weights, x)
```

### 4. æ··åˆæ³¨æ„åŠ›ç›‘ç£

```python
from med_core.attention_supervision import HybridAttentionSupervision

# åˆ›å»ºæ··åˆç›‘ç£
supervision = HybridAttentionSupervision(
    loss_weight=0.1,
    channel_weight=1.0,
    spatial_weight=1.0,
    transformer_weight=1.0,
)

# æ”¶é›†å¤šç§æ³¨æ„åŠ›æƒé‡
attentions = {
    "channel": channel_weights,    # (B, C)
    "spatial": spatial_weights,    # (B, 1, H, W)
    "transformer": transformer_weights,  # (B, num_heads, N, N)
}

# è®¡ç®—ç›‘ç£æŸå¤±
loss = supervision(attentions, features)
```

---

## å®Œæ•´ç¤ºä¾‹

### ç¤ºä¾‹ 1: åœ¨ ResNet ä¸­æ·»åŠ  SE æ³¨æ„åŠ›

```python
import torch.nn as nn
from med_core.attention_supervision import SEAttention

class SEResNetBlock(nn.Module):
    def __init__(self, channels):
        super().__init__()
        self.conv1 = nn.Conv2d(channels, channels, 3, padding=1)
        self.bn1 = nn.BatchNorm2d(channels)
        self.conv2 = nn.Conv2d(channels, channels, 3, padding=1)
        self.bn2 = nn.BatchNorm2d(channels)
        self.se = SEAttention(channels, reduction=16)
        self.relu = nn.ReLU(inplace=True)
    
    def forward(self, x):
        identity = x
        
        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)
        
        out = self.conv2(out)
        out = self.bn2(out)
        
        # åº”ç”¨ SE æ³¨æ„åŠ›
        out = self.se(out)
        
        out += identity
        out = self.relu(out)
        
        return out
```

### ç¤ºä¾‹ 2: å¸¦æ³¨æ„åŠ›ç›‘ç£çš„åˆ†ç±»æ¨¡å‹

```python
import torch.nn as nn
from med_core.attention_supervision import (
    SEAttention,
    ChannelAttentionSupervision,
)

class AttentionClassifier(nn.Module):
    def __init__(self, num_classes=10):
        super().__init__()
        
        # éª¨å¹²ç½‘ç»œ
        self.backbone = nn.Sequential(
            nn.Conv2d(3, 64, 3, padding=1),
            nn.ReLU(),
            nn.Conv2d(64, 256, 3, padding=1),
            nn.ReLU(),
        )
        
        # SE æ³¨æ„åŠ›
        self.attention = SEAttention(channels=256, reduction=16)
        
        # æ³¨æ„åŠ›ç›‘ç£
        self.attention_supervision = ChannelAttentionSupervision(
            loss_weight=0.1,
            diversity_weight=0.1,
        )
        
        # åˆ†ç±»å¤´
        self.classifier = nn.Sequential(
            nn.AdaptiveAvgPool2d(1),
            nn.Flatten(),
            nn.Linear(256, num_classes),
        )
    
    def forward(self, x, return_attention=False):
        # ç‰¹å¾æå–
        features = self.backbone(x)
        
        # åº”ç”¨æ³¨æ„åŠ›
        attended_features = self.attention(features)
        
        # åˆ†ç±»
        logits = self.classifier(attended_features)
        
        if return_attention:
            attn_weights = self.attention.get_attention_weights(features)
            return logits, attn_weights
        
        return logits
    
    def compute_loss(self, x, y):
        # å‰å‘ä¼ æ’­
        logits, attn_weights = self.forward(x, return_attention=True)
        
        # åˆ†ç±»æŸå¤±
        cls_loss = nn.CrossEntropyLoss()(logits, y)
        
        # æ³¨æ„åŠ›ç›‘ç£æŸå¤±
        features = self.backbone(x)
        attn_loss = self.attention_supervision(attn_weights, features)
        
        # æ€»æŸå¤±
        total_loss = cls_loss + attn_loss.total_loss
        
        return total_loss, {
            "cls_loss": cls_loss.item(),
            "attn_loss": attn_loss.total_loss.item(),
            **{k: v.item() for k, v in attn_loss.components.items()},
        }

# ä½¿ç”¨
model = AttentionClassifier(num_classes=10)
x = torch.randn(2, 3, 224, 224)
y = torch.randint(0, 10, (2,))

loss, loss_dict = model.compute_loss(x, y)
print(f"Total loss: {loss.item():.4f}")
print(f"Loss components: {loss_dict}")
```

---

## æ€§èƒ½å¯¹æ¯”

| æ³¨æ„åŠ›ç±»å‹ | å‚æ•°é‡ | è®¡ç®—é‡ | æ€§èƒ½ | é€‚ç”¨åœºæ™¯ |
|-----------|--------|--------|------|---------|
| SE | ä¸­ | ä½ | å¥½ | é€šç”¨ |
| ECA | ä½ | ä½ | ä¼˜ | æ•ˆç‡ä¼˜å…ˆ |
| Spatial | ä½ | ä½ | ä¸­ | ç›®æ ‡å®šä½ |
| CBAM | ä¸­ | ä¸­ | ä¼˜ | æ€§èƒ½ä¼˜å…ˆ |
| Transformer | é«˜ | é«˜ | ä¼˜ | å…¨å±€å»ºæ¨¡ |

---

## æœ€ä½³å®è·µ

### 1. é€‰æ‹©åˆé€‚çš„æ³¨æ„åŠ›ç±»å‹

- **é€šé“æ³¨æ„åŠ› (SE/ECA)**: é€‚åˆå¢å¼ºé‡è¦ç‰¹å¾é€šé“
- **ç©ºé—´æ³¨æ„åŠ›**: é€‚åˆç›®æ ‡å®šä½å’Œæ˜¾è‘—æ€§æ£€æµ‹
- **CBAM**: é€‚åˆéœ€è¦åŒæ—¶å…³æ³¨é€šé“å’Œç©ºé—´çš„åœºæ™¯
- **Transformer**: é€‚åˆéœ€è¦å…¨å±€å»ºæ¨¡çš„åœºæ™¯

### 2. æ³¨æ„åŠ›ä½ç½®

- **æµ…å±‚**: å…³æ³¨ä½çº§ç‰¹å¾ï¼ˆè¾¹ç¼˜ã€çº¹ç†ï¼‰
- **æ·±å±‚**: å…³æ³¨é«˜çº§è¯­ä¹‰ç‰¹å¾
- **å¤šå±‚**: åœ¨å¤šä¸ªå±‚çº§æ·»åŠ æ³¨æ„åŠ›

### 3. è¶…å‚æ•°è°ƒä¼˜

- **reduction**: SE çš„é™ç»´æ¯”ä¾‹ï¼Œé€šå¸¸ 8-16
- **num_heads**: Transformer çš„å¤´æ•°ï¼Œé€šå¸¸ 4-16
- **loss_weight**: æ³¨æ„åŠ›ç›‘ç£çš„æƒé‡ï¼Œé€šå¸¸ 0.01-0.1

### 4. è®­ç»ƒæŠ€å·§

- å…ˆè®­ç»ƒä¸»ä»»åŠ¡ï¼Œå†æ·»åŠ æ³¨æ„åŠ›ç›‘ç£
- ä½¿ç”¨è¾ƒå°çš„å­¦ä¹ ç‡è®­ç»ƒæ³¨æ„åŠ›æ¨¡å—
- ç›‘æ§æ³¨æ„åŠ›æƒé‡çš„åˆ†å¸ƒ

---

## å¸¸è§é—®é¢˜

### Q1: å¦‚ä½•é€‰æ‹©æ³¨æ„åŠ›ç±»å‹ï¼Ÿ

**A**: æ ¹æ®ä»»åŠ¡éœ€æ±‚ï¼š
- åˆ†ç±»ä»»åŠ¡ï¼šSE æˆ– ECA
- æ£€æµ‹ä»»åŠ¡ï¼šCBAM æˆ– Spatial
- åˆ†å‰²ä»»åŠ¡ï¼šSpatial æˆ– Transformer
- é€šç”¨åœºæ™¯ï¼šCBAM

### Q2: æ³¨æ„åŠ›ç›‘ç£æ˜¯å¦å¿…éœ€ï¼Ÿ

**A**: ä¸æ˜¯å¿…éœ€çš„ï¼Œä½†æ¨èä½¿ç”¨ï¼š
- æé«˜å¯è§£é‡Šæ€§
- æ”¹å–„å°æ ·æœ¬å­¦ä¹ 
- åŠ é€Ÿæ”¶æ•›

### Q3: å¦‚ä½•å¯è§†åŒ–æ³¨æ„åŠ›ï¼Ÿ

**A**: ä½¿ç”¨ `get_attention_weights()` æ–¹æ³•ï¼š

```python
# è·å–æ³¨æ„åŠ›æƒé‡
weights = attention.get_attention_weights(x)

# å¯è§†åŒ–
import matplotlib.pyplot as plt
plt.imshow(weights[0].detach().cpu().numpy())
plt.colorbar()
plt.show()
```

### Q4: æ³¨æ„åŠ›æ¨¡å—ä¼šå¢åŠ å¤šå°‘è®¡ç®—é‡ï¼Ÿ

**A**: 
- SE/ECA: <1% é¢å¤–è®¡ç®—
- Spatial: <1% é¢å¤–è®¡ç®—
- CBAM: ~1-2% é¢å¤–è®¡ç®—
- Transformer: 5-10% é¢å¤–è®¡ç®—

---

## å‚è€ƒèµ„æº

### è®ºæ–‡

1. SE-Net: Hu et al. "Squeeze-and-Excitation Networks" CVPR 2018
2. ECA-Net: Wang et al. "ECA-Net: Efficient Channel Attention for Deep CNNs" CVPR 2020
3. CBAM: Woo et al. "CBAM: Convolutional Block Attention Module" ECCV 2018
4. Transformer: Vaswani et al. "Attention Is All You Need" NeurIPS 2017

### ä»£ç 

- `med_core/attention_supervision/advanced_attention.py` - æ³¨æ„åŠ›æ¨¡å—å®ç°
- `med_core/attention_supervision/advanced_supervision.py` - æ³¨æ„åŠ›ç›‘ç£å®ç°
- `examples/advanced_attention_demo.py` - ä½¿ç”¨ç¤ºä¾‹
- `tests/test_advanced_attention.py` - å•å…ƒæµ‹è¯•

### æ–‡æ¡£

- [API æ–‡æ¡£](../api/attention_supervision.md)
- [æ³¨æ„åŠ›ç›‘ç£æŒ‡å—](attention_supervision.md)
- [æ€§èƒ½ä¼˜åŒ–æŒ‡å—](performance_optimization.md)

---

## æ›´æ–°æ—¥å¿—

### v0.2.0 (2026-02-20)

- âœ¨ æ–°å¢ SE æ³¨æ„åŠ›
- âœ¨ æ–°å¢ ECA æ³¨æ„åŠ›
- âœ¨ æ–°å¢ç©ºé—´æ³¨æ„åŠ›
- âœ¨ æ–°å¢ CBAM
- âœ¨ æ–°å¢ Transformer æ³¨æ„åŠ›
- âœ¨ æ–°å¢é€šé“æ³¨æ„åŠ›ç›‘ç£
- âœ¨ æ–°å¢ç©ºé—´æ³¨æ„åŠ›ç›‘ç£
- âœ¨ æ–°å¢ Transformer æ³¨æ„åŠ›ç›‘ç£
- âœ¨ æ–°å¢æ··åˆæ³¨æ„åŠ›ç›‘ç£
- âœ¨ æ–°å¢å·¥å‚å‡½æ•°
- ğŸ“ å®Œå–„æ–‡æ¡£å’Œç¤ºä¾‹
- âœ… æ·»åŠ å®Œæ•´çš„å•å…ƒæµ‹è¯•
