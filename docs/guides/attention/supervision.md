# ç¦»çº¿æ³¨æ„åŠ›å¼•å¯¼ä½¿ç”¨æŒ‡å—

æœ¬æŒ‡å—ä»‹ç»å¦‚ä½•åœ¨ Med-Framework ä¸­ä½¿ç”¨ç¦»çº¿æ³¨æ„åŠ›å¼•å¯¼åŠŸèƒ½ï¼Œåœ¨è®­ç»ƒé˜¶æ®µå°±è®©æ¨¡å‹å­¦ä¼šå…³æ³¨æ­£ç¡®çš„åŒºåŸŸã€‚

---

## ğŸ“‹ ç›®å½•

1. [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
2. [æ–¹æ³•é€‰æ‹©](#æ–¹æ³•é€‰æ‹©)
3. [è¯¦ç»†ç¤ºä¾‹](#è¯¦ç»†ç¤ºä¾‹)
4. [é…ç½®è¯´æ˜](#é…ç½®è¯´æ˜)
5. [å¯è§†åŒ–](#å¯è§†åŒ–)
6. [æœ€ä½³å®è·µ](#æœ€ä½³å®è·µ)

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### æ–¹æ³•1: ä½¿ç”¨åˆ†å‰²æ©ç ç›‘ç£ï¼ˆæ¨èï¼Œå¦‚æœæœ‰æ©ç ï¼‰

```python
import torch
from med_core.datasets.attention_supervised import MedicalAttentionSupervisedDataset
from med_core.attention_supervision import MaskSupervisedAttention
from med_core.configs.attention_config import create_mask_supervised_config

# 1. åˆ›å»ºæ•°æ®é›†ï¼ˆæ”¯æŒæ©ç åŠ è½½ï¼‰
dataset = MedicalAttentionSupervisedDataset.from_csv(
    csv_path="data/annotations.csv",
    image_dir="data/images/",
    mask_dir="data/masks/",  # ğŸ‘ˆ æ©ç ç›®å½•
    image_col="scan_path",
    mask_col="lesion_mask",
    label_col="diagnosis",
    tabular_cols=["age", "gender", "symptoms"],
    return_mask=True,  # ğŸ‘ˆ è¿”å›æ©ç 
)

# 2. åˆ›å»ºæ³¨æ„åŠ›ç›‘ç£æ¨¡å—
attention_supervision = MaskSupervisedAttention(
    loss_weight=0.1,
    loss_type="kl",
    temperature=10.0,
)

# 3. è®­ç»ƒå¾ªç¯
for images, tabular, labels, masks in dataloader:
    # å‰å‘ä¼ æ’­
    outputs = model(images, tabular)
    attention = outputs["attention_weights"]  # æ¨¡å‹çš„æ³¨æ„åŠ›æƒé‡
    
    # è®¡ç®—åˆ†ç±»æŸå¤±
    classification_loss = criterion(outputs["logits"], labels)
    
    # è®¡ç®—æ³¨æ„åŠ›ç›‘ç£æŸå¤±
    attention_loss_result = attention_supervision(
        attention_weights=attention,
        features=outputs["features"],
        targets=masks,  # ğŸ‘ˆ ä½¿ç”¨æ©ç ç›‘ç£
    )
    
    # æ€»æŸå¤±
    total_loss = classification_loss + attention_loss_result.total_loss
    
    # åå‘ä¼ æ’­
    total_loss.backward()
    optimizer.step()
```

### æ–¹æ³•2: ä½¿ç”¨ CAM è‡ªç›‘ç£ï¼ˆæ¨èï¼Œåªæœ‰å›¾åƒæ ‡ç­¾ï¼‰

```python
from med_core.attention_supervision import CAMSelfSupervision

# åˆ›å»º CAM è‡ªç›‘ç£æ¨¡å—
attention_supervision = CAMSelfSupervision(
    loss_weight=0.1,
    consistency_method="entropy",
    alignment_weight=0.5,
)

# è®­ç»ƒå¾ªç¯
for images, tabular, labels in dataloader:  # ğŸ‘ˆ ä¸éœ€è¦æ©ç 
    outputs = model(images, tabular)
    
    classification_loss = criterion(outputs["logits"], labels)
    
    # CAM è‡ªç›‘ç£
    attention_loss_result = attention_supervision(
        attention_weights=outputs["attention_weights"],
        features=outputs["features"],
        classifier_weights=model.classifier.weight,  # ğŸ‘ˆ åˆ†ç±»å™¨æƒé‡
        predicted_class=outputs["logits"].argmax(dim=1),
    )
    
    total_loss = classification_loss + attention_loss_result.total_loss
    total_loss.backward()
    optimizer.step()
```

---

## ğŸ¯ æ–¹æ³•é€‰æ‹©

æ ¹æ®ä½ çš„ï¿½ï¿½ï¿½æ®é›†æ ‡æ³¨æƒ…å†µé€‰æ‹©åˆé€‚çš„æ–¹æ³•ï¼š

| æ•°æ®é›†æ ‡æ³¨ | æ¨èæ–¹æ³• | ä¼˜å…ˆçº§ | æ•ˆæœ |
|-----------|---------|--------|------|
| âœ… æœ‰åˆ†å‰²æ©ç  | åˆ†å‰²æ©ç ç›‘ç£ | â­â­â­â­â­ | æœ€å¥½ |
| âœ… æœ‰è¾¹ç•Œæ¡† | è¾¹ç•Œæ¡†ç›‘ç£ | â­â­â­â­ | å¾ˆå¥½ |
| âœ… æœ‰å…³é”®ç‚¹ | å…³é”®ç‚¹ç›‘ç£ | â­â­â­ | å¥½ |
| âŒ åªæœ‰å›¾åƒæ ‡ç­¾ | CAM è‡ªç›‘ç£ | â­â­â­â­ | å¥½ |
| âŒ åªæœ‰å›¾åƒæ ‡ç­¾ | å¤šå®ä¾‹å­¦ä¹  | â­â­â­â­ | å¥½ |

### æ£€æŸ¥æ•°æ®é›†æ ‡æ³¨

```python
# æ£€æŸ¥ä½ çš„æ•°æ®é›†æœ‰ä»€ä¹ˆæ ‡æ³¨
dataset = MedicalAttentionSupervisedDataset.from_csv(...)

print(f"æœ‰åˆ†å‰²æ©ç : {dataset.has_masks()}")
print(f"æœ‰è¾¹ç•Œæ¡†: {dataset.has_bboxes()}")
print(f"æœ‰å…³é”®ç‚¹: {dataset.has_keypoints()}")

if dataset.has_masks():
    print(f"æ©ç è¦†ç›–ç‡: {dataset.get_mask_coverage():.1%}")
```

---

## ğŸ“š è¯¦ç»†ç¤ºä¾‹

### ç¤ºä¾‹1: è‚ºç‚æ£€æµ‹ï¼ˆåˆ†å‰²æ©ç ç›‘ç£ï¼‰

```python
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from torchvision import transforms

from med_core.datasets.attention_supervised import MedicalAttentionSupervisedDataset
from med_core.attention_supervision import MaskSupervisedAttention
from med_core.visualization.attention_viz import visualize_attention_supervision_loss

# 1. æ•°æ®å‡†å¤‡
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

dataset = MedicalAttentionSupervisedDataset.from_csv(
    csv_path="data/pneumonia.csv",
    image_dir="data/chest_xrays/",
    mask_dir="data/lesion_masks/",
    image_col="xray_path",
    mask_col="lesion_mask",
    label_col="has_pneumonia",
    tabular_cols=["age", "gender", "fever", "cough"],
    image_format="png",
    transform=transform,
    return_mask=True,
)

dataloader = DataLoader(dataset, batch_size=32, shuffle=True)

# 2. æ¨¡å‹å®šä¹‰ï¼ˆå‡è®¾ä½ å·²æœ‰æ¨¡å‹ï¼‰
class PneumoniaModel(nn.Module):
    def __init__(self):
        super().__init__()
        # ... ä½ çš„æ¨¡å‹å®šä¹‰
        self.attention_module = nn.Sequential(
            nn.Conv2d(512, 1, kernel_size=1),
            nn.Sigmoid(),
        )
    
    def forward(self, images, tabular):
        features = self.backbone(images)  # (B, 512, 7, 7)
        attention = self.attention_module(features)  # (B, 1, 7, 7)
        
        # åŠ æƒç‰¹å¾
        weighted_features = features * attention
        pooled = F.adaptive_avg_pool2d(weighted_features, 1).flatten(1)
        
        # èåˆè¡¨æ ¼æ•°æ®
        combined = torch.cat([pooled, tabular], dim=1)
        logits = self.classifier(combined)
        
        return {
            "logits": logits,
            "attention_weights": attention.squeeze(1),
            "features": features,
        }

model = PneumoniaModel()

# 3. æ³¨æ„åŠ›ç›‘ç£
attention_supervision = MaskSupervisedAttention(
    loss_weight=0.1,
    loss_type="kl",
    temperature=10.0,
    add_smooth_loss=True,
    smooth_weight=0.01,
)

# 4. è®­ç»ƒ
optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)
criterion = nn.CrossEntropyLoss()

model.train()
for epoch in range(100):
    for batch_idx, (images, tabular, labels, masks) in enumerate(dataloader):
        images = images.cuda()
        tabular = tabular.cuda()
        labels = labels.cuda()
        masks = masks.cuda()
        
        # å‰å‘ä¼ æ’­
        outputs = model(images, tabular)
        
        # åˆ†ç±»æŸå¤±
        classification_loss = criterion(outputs["logits"], labels)
        
        # æ³¨æ„åŠ›ç›‘ç£æŸå¤±
        attention_loss_result = attention_supervision(
            attention_weights=outputs["attention_weights"],
            features=outputs["features"],
            targets=masks,
        )
        
        # æ€»æŸå¤±
        total_loss = classification_loss + attention_loss_result.total_loss
        
        # åå‘ä¼ æ’­
        optimizer.zero_grad()
        total_loss.backward()
        optimizer.step()
        
        # æ—¥å¿—
        if batch_idx % 10 == 0:
            print(f"Epoch {epoch}, Batch {batch_idx}")
            print(f"  åˆ†ç±»æŸå¤±: {classification_loss.item():.4f}")
            print(f"  æ³¨æ„åŠ›æŸå¤±: {attention_loss_result.total_loss.item():.4f}")
            for name, value in attention_loss_result.components.items():
                print(f"    {name}: {value.item():.4f}")
        
        # å¯è§†åŒ–ï¼ˆæ¯100æ­¥ï¼‰
        if batch_idx % 100 == 0:
            fig = visualize_attention_supervision_loss(
                image=images[0],
                attention=attention_loss_result.attention_weights[0],
                target=attention_loss_result.metadata["target"][0],
                loss_components={k: v.item() for k, v in attention_loss_result.components.items()},
                save_path=f"outputs/attention_epoch{epoch}_batch{batch_idx}.png",
            )
            plt.close(fig)
```

### ç¤ºä¾‹2: è‚ºç»“èŠ‚æ£€æµ‹ï¼ˆCAM è‡ªç›‘ç£ï¼‰

```python
from med_core.attention_supervision import CAMSelfSupervision

# æ•°æ®é›†ï¼ˆåªæœ‰å›¾åƒæ ‡ç­¾ï¼Œæ²¡æœ‰æ©ç ï¼‰
dataset = MedicalAttentionSupervisedDataset.from_csv(
    csv_path="data/nodules.csv",
    image_dir="data/ct_scans/",
    mask_dir=None,  # ğŸ‘ˆ æ²¡æœ‰æ©ç 
    label_col="has_nodule",
    tabular_cols=["age", "smoking_history"],
    return_mask=False,  # ğŸ‘ˆ ä¸è¿”å›æ©ç 
)

# CAM è‡ªç›‘ç£
attention_supervision = CAMSelfSupervision(
    loss_weight=0.1,
    consistency_method="entropy",
    consistency_weight=1.0,
    alignment_weight=0.5,
)

# è®­ç»ƒ
for images, tabular, labels in dataloader:  # ğŸ‘ˆ æ²¡æœ‰æ©ç 
    outputs = model(images, tabular)
    
    classification_loss = criterion(outputs["logits"], labels)
    
    # CAM è‡ªç›‘ç£
    attention_loss_result = attention_supervision(
        attention_weights=outputs["attention_weights"],
        features=outputs["features"],
        classifier_weights=model.classifier.weight,
        predicted_class=outputs["logits"].argmax(dim=1),
    )
    
    total_loss = classification_loss + attention_loss_result.total_loss
    total_loss.backward()
    optimizer.step()
    
    # å¯è§†åŒ– CAM
    if batch_idx % 100 == 0:
        cam = attention_loss_result.metadata["cam"][0]
        fig = visualize_attention_overlay(
            image=images[0],
            attention=cam,
            title="CAM å¯è§†åŒ–",
            save_path=f"outputs/cam_batch{batch_idx}.png",
        )
        plt.close(fig)
```

### ç¤ºä¾‹3: å¤šå®ä¾‹å­¦ä¹ ï¼ˆMILï¼‰

```python
from med_core.attention_supervision import AttentionMIL, MILSupervision
from med_core.visualization.attention_viz import visualize_mil_attention

# åˆ›å»º MIL æ¨¡å‹
from torchvision.models import resnet18
backbone = resnet18(pretrained=True)
backbone = nn.Sequential(*list(backbone.children())[:-2])  # ç§»é™¤åˆ†ç±»å±‚

mil_model = AttentionMIL(
    backbone=backbone,
    feature_dim=512,
    num_classes=2,
    patch_size=16,
    attention_dim=128,
    pooling_mode="attention",
)

# MIL ç›‘ç£
mil_supervision = MILSupervision(
    loss_weight=0.1,
    patch_size=16,
    diversity_weight=0.1,
)

# è®­ç»ƒ
for images, tabular, labels in dataloader:
    # MIL å‰å‘ä¼ æ’­
    mil_outputs = mil_model(images)
    
    # åˆ†ç±»æŸå¤±
    classification_loss = criterion(mil_outputs["logits"], labels)
    
    # MIL ç›‘ç£æŸå¤±
    attention_loss_result = mil_supervision(
        attention_weights=mil_outputs["attention_weights"],
        features=mil_outputs["patch_features"],
        grid_size=mil_outputs["grid_size"],
    )
    
    total_loss = classification_loss + attention_loss_result.total_loss
    total_loss.backward()
    optimizer.step()
    
    # å¯è§†åŒ– MIL æ³¨æ„åŠ›
    if batch_idx % 100 == 0:
        fig = visualize_mil_attention(
            image=images[0],
            patch_attention=mil_outputs["attention_weights"][0],
            grid_size=mil_outputs["grid_size"],
            top_k=5,
            save_path=f"outputs/mil_batch{batch_idx}.png",
        )
        plt.close(fig)
```

### ç¤ºä¾‹4: è¾¹ç•Œæ¡†ç›‘ç£

```python
from med_core.attention_supervision import BBoxSupervisedAttention

# æ•°æ®é›†ï¼ˆæœ‰è¾¹ç•Œæ¡†æ ‡æ³¨ï¼‰
dataset = AttentionSupervisedDataset(
    image_paths=image_paths,
    tabular_data=tabular_data,
    labels=labels,
    bboxes=bboxes,  # ğŸ‘ˆ è¾¹ç•Œæ¡†åˆ—è¡¨ [[x_min, y_min, x_max, y_max], ...]
    return_bbox=True,
)

# è¾¹ç•Œæ¡†ç›‘ç£
attention_supervision = BBoxSupervisedAttention(
    loss_weight=0.1,
    bbox_format="xyxy",
)

# è®­ç»ƒ
for images, tabular, labels, bboxes in dataloader:
    outputs = model(images, tabular)
    
    classification_loss = criterion(outputs["logits"], labels)
    
    # è¾¹ç•Œæ¡†ç›‘ç£
    attention_loss_result = attention_supervision(
        attention_weights=outputs["attention_weights"],
        features=outputs["features"],
        targets=bboxes,  # ğŸ‘ˆ è¾¹ç•Œæ¡†
        image_size=(512, 512),  # åŸå›¾å°ºå¯¸
    )
    
    total_loss = classification_loss + attention_loss_result.total_loss
    total_loss.backward()
    optimizer.step()
```

---

## âš™ï¸ é…ç½®è¯´æ˜

### ä½¿ç”¨é…ç½®æ–‡ä»¶

```python
from med_core.configs.attention_config import (
    ExperimentConfigWithAttention,
    DataConfigWithMask,
    TrainingConfigWithAttention,
    AttentionSupervisionConfig,
)

# åˆ›å»ºå®Œæ•´é…ç½®
config = ExperimentConfigWithAttention(
    experiment_name="pneumonia_detection_with_attention",
    output_dir="outputs/",
    
    # æ•°æ®é…ç½®
    data=DataConfigWithMask(
        csv_file="data/pneumonia.csv",
        image_dir="data/images/",
        mask_dir="data/masks/",
        return_mask=True,
    ),
    
    # è®­ç»ƒé…ç½®
    training=TrainingConfigWithAttention(
        num_epochs=100,
        batch_size=32,
        learning_rate=1e-4,
        
        # æ³¨æ„åŠ›ç›‘ç£é…ç½®
        attention_supervision=AttentionSupervisionConfig(
            enabled=True,
            method="mask",
            loss_weight=0.1,
            loss_type="kl",
            temperature=10.0,
        ),
        
        log_attention_every=100,
        save_attention_maps=True,
    ),
)

# ä¿å­˜é…ç½®
import yaml
with open("config.yaml", "w") as f:
    yaml.dump(config.__dict__, f)
```

### é¢„è®¾é…ç½®

```python
from med_core.configs.attention_config import (
    create_mask_supervised_config,
    create_cam_supervised_config,
    create_mil_config,
    create_bbox_supervised_config,
)

# åˆ†å‰²æ©ç ç›‘ç£é…ç½®
mask_config = create_mask_supervised_config(
    loss_weight=0.1,
    loss_type="kl",
)

# CAM è‡ªç›‘ç£é…ç½®
cam_config = create_cam_supervised_config(
    loss_weight=0.1,
    consistency_method="entropy",
)

# MIL é…ç½®
mil_config = create_mil_config(
    loss_weight=0.1,
    patch_size=16,
)

# è¾¹ç•Œæ¡†ç›‘ç£é…ç½®
bbox_config = create_bbox_supervised_config(
    loss_weight=0.1,
    bbox_format="xyxy",
)
```

---

## ğŸ“Š å¯è§†åŒ–

### å¯è§†åŒ–æ³¨æ„åŠ›å åŠ 

```python
from med_core.visualization.attention_viz import visualize_attention_overlay

fig = visualize_attention_overlay(
    image=image,
    attention=attention_weights,
    alpha=0.5,
    cmap="jet",
    title="æ³¨æ„åŠ›å¯è§†åŒ–",
    save_path="attention.png",
)
```

### å¯è§†åŒ–ç›‘ç£æ•ˆæœ

```python
from med_core.visualization.attention_viz import visualize_attention_comparison

fig = visualize_attention_comparison(
    image=image,
    attention_before=attention_before_supervision,
    attention_after=attention_after_supervision,
    target=mask,
    titles=["åŸå›¾", "ç›‘ç£å‰", "ç›‘ç£å", "ç›®æ ‡æ©ç "],
    save_path="comparison.png",
)
```

### å¯è§†åŒ–æŸå¤±ç»„ä»¶

```python
from med_core.visualization.attention_viz import visualize_attention_supervision_loss

fig = visualize_attention_supervision_loss(
    image=image,
    attention=attention_weights,
    target=mask,
    loss_components={"main": 0.5, "smooth": 0.1},
    save_path="loss.png",
)
```

### å¯è§†åŒ– MIL æ³¨æ„åŠ›

```python
from med_core.visualization.attention_viz import visualize_mil_attention

fig = visualize_mil_attention(
    image=image,
    patch_attention=patch_attention_weights,
    grid_size=(14, 14),
    top_k=5,
    save_path="mil_attention.png",
)
```

---

## ğŸ’¡ æœ€ä½³å®è·µ

### 1. æŸå¤±æƒé‡è°ƒæ•´

```python
# å¼€å§‹æ—¶ä½¿ç”¨è¾ƒå°çš„æƒé‡
attention_supervision = MaskSupervisedAttention(
    loss_weight=0.01,  # ğŸ‘ˆ ä»å°å¼€å§‹
)

# è®­ç»ƒç¨³å®šåé€æ¸å¢åŠ 
for epoch in range(100):
    if epoch > 20:
        attention_supervision.loss_weight = 0.1  # å¢åŠ æƒé‡
```

### 2. æ¸è¿›å¼è®­ç»ƒ

```python
# å‰å‡ ä¸ª epoch ä¸ä½¿ç”¨æ³¨æ„åŠ›ç›‘ç£
attention_supervision = MaskSupervisedAttention(
    loss_weight=0.1,
    enabled=False,  # ğŸ‘ˆ å…ˆç¦ç”¨
)

for epoch in range(100):
    if epoch >= 10:
        attention_supervision.enabled = True  # ğŸ‘ˆ 10 ä¸ª epoch åå¯ç”¨
    
    # è®­ç»ƒ...
```

### 3. ç›‘æ§æ³¨æ„åŠ›è´¨é‡

```python
from med_core.visualization.attention_viz import plot_attention_statistics

attention_history = []

for epoch in range(100):
    for batch in dataloader:
        # è®­ç»ƒ...
        attention_history.append(outputs["attention_weights"][0])
    
    # æ¯ä¸ª epoch ç»“æŸåç»˜åˆ¶ç»Ÿè®¡
    if epoch % 10 == 0:
        fig = plot_attention_statistics(
            attention_history[-100:],  # æœ€è¿‘ 100 æ­¥
            save_path=f"outputs/stats_epoch{epoch}.png",
        )
        plt.close(fig)
```

### 4. æ•°æ®å¢å¼ºæ³¨æ„äº‹é¡¹

```python
# å›¾åƒå’Œæ©ç éœ€è¦ä½¿ç”¨ç›¸åŒçš„å˜æ¢
from torchvision import transforms

# å®šä¹‰å˜æ¢
image_transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.RandomHorizontalFlip(),
    transforms.ToTensor(),
])

# æ©ç å˜æ¢ï¼ˆä¸åŒ…æ‹¬å½’ä¸€åŒ–ï¼‰
mask_transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.RandomHorizontalFlip(),  # ğŸ‘ˆ ä¸å›¾åƒç›¸åŒçš„éšæœºå˜æ¢
    transforms.ToTensor(),
])

dataset = MedicalAttentionSupervisedDataset(
    ...,
    transform=image_transform,
    mask_transform=mask_transform,
)
```

### 5. æ··åˆä½¿ç”¨å¤šç§æ–¹æ³•

```python
# å¦‚æœéƒ¨åˆ†æ ·æœ¬æœ‰æ©ç ï¼Œéƒ¨åˆ†æ²¡æœ‰
for images, tabular, labels, masks in dataloader:
    outputs = model(images, tabular)
    classification_loss = criterion(outputs["logits"], labels)
    
    # æ£€æŸ¥å“ªäº›æ ·æœ¬æœ‰æ©ç 
    has_mask = masks is not None and not torch.all(masks == 0)
    
    if has_mask:
        # ä½¿ç”¨æ©ç ç›‘ç£
        attention_loss = mask_supervision(
            attention_weights=outputs["attention_weights"],
            features=outputs["features"],
            targets=masks,
        )
    else:
        # ä½¿ç”¨ CAM è‡ªç›‘ç£
        attention_loss = cam_supervision(
            attention_weights=outputs["attention_weights"],
            features=outputs["features"],
            classifier_weights=model.classifier.weight,
        )
    
    total_loss = classification_loss + attention_loss.total_loss
    total_loss.backward()
    optimizer.step()
```

---

## ğŸ” æ•…éšœæ’é™¤

### é—®é¢˜1: æ³¨æ„åŠ›æŸå¤±è¿‡å¤§

**åŸå› **: æŸå¤±æƒé‡å¤ªå¤§æˆ–æ¸©åº¦å‚æ•°ä¸åˆé€‚

**è§£å†³æ–¹æ¡ˆ**:
```python
# å‡å°æŸå¤±æƒé‡
attention_supervision.loss_weight = 0.01  # ä» 0.1 é™åˆ° 0.01

# è°ƒæ•´æ¸©åº¦å‚æ•°
attention_supervision.temperature = 5.0  # ä» 10.0 é™åˆ° 5.0
```

### é—®é¢˜2: æ³¨æ„åŠ›ä¸é›†ä¸­

**åŸå› **: ä¸€è‡´æ€§æŸå¤±æƒé‡å¤ªå°

**è§£å†³æ–¹æ¡ˆ**:
```python
# å¢åŠ ä¸€è‡´æ€§æŸå¤±æƒé‡
cam_supervision = CAMSelfSupervision(
    consistency_weight=2.0,  # ä» 1.0 å¢åŠ åˆ° 2.0
)
```

### é—®é¢˜3: è®­ç»ƒä¸ç¨³å®š

**åŸå› **: æ³¨æ„åŠ›ç›‘ç£è¿‡æ—©å¼•å…¥

**è§£å†³æ–¹æ¡ˆ**:
```python
# å»¶è¿Ÿå¯ç”¨æ³¨æ„åŠ›ç›‘ç£
for epoch in range(100):
    if epoch < 20:
        attention_supervision.enabled = False
    else:
        attention_supervision.enabled = True
```

---

## ğŸ“– å‚è€ƒèµ„æ–™

- [ç¦»çº¿æ³¨æ„åŠ›å¼•å¯¼æ–¹æ¡ˆæ–‡æ¡£](./offline-attention-guidance.md)
- [äº¤äº’å¼å¼•å¯¼è·¯çº¿å›¾](./interactive-guidance-roadmap.md)
- [å†³ç­–é“¾ç ”ç©¶æŠ¥å‘Š](./decision-chain-research.md)

---

**æœ€åæ›´æ–°**: 2026-02-13  
**ç‰ˆæœ¬**: v1.0
