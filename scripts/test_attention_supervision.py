"""
æµ‹è¯•æ³¨æ„åŠ›ç›‘ç£çš„å®é™…æ•ˆæœ

å¯¹æ¯”å®éªŒï¼š
1. åŸºçº¿æ¨¡å‹ï¼ˆæ— æ³¨æ„åŠ›ç›‘ç£ï¼‰
2. CAM æ³¨æ„åŠ›ç›‘ç£æ¨¡å‹

ç›®æ ‡ï¼šéªŒè¯æ³¨æ„åŠ›ç›‘ç£æ˜¯å¦çœŸçš„èƒ½æå‡å‡†ç¡®ç‡
"""

import logging
import shutil
import sys
from pathlib import Path

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from PIL import Image

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from med_core.backbones import create_tabular_backbone, create_vision_backbone
from med_core.datasets import (
    MedicalMultimodalDataset,
    create_dataloaders,
    get_train_transforms,
    get_val_transforms,
    split_dataset,
)
from med_core.evaluation import calculate_binary_metrics
from med_core.fusion import MultiModalFusionModel, create_fusion_module

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
)
logger = logging.getLogger(__name__)


def generate_synthetic_data(output_dir: Path, num_samples: int = 200):
    """ç”Ÿæˆåˆæˆæµ‹è¯•æ•°æ®"""
    logger.info(f"ç”Ÿæˆ {num_samples} æ¡åˆæˆæ•°æ®...")

    image_dir = output_dir / "images"
    image_dir.mkdir(parents=True, exist_ok=True)

    data = []

    for i in range(num_samples):
        # ç”Ÿæˆéšæœºå›¾ç‰‡
        img_array = np.random.randint(0, 255, (224, 224, 3), dtype=np.uint8)

        # ä¸ºæ­£ç±»æ·»åŠ ç‰¹å¾ï¼ˆç™½è‰²åœ†åœˆï¼‰
        label = np.random.randint(0, 2)
        if label == 1:
            center = (np.random.randint(50, 174), np.random.randint(50, 174))
            radius = np.random.randint(10, 30)
            y, x = np.ogrid[:224, :224]
            mask = (x - center[0]) ** 2 + (y - center[1]) ** 2 <= radius**2
            img_array[mask] = 255

        img = Image.fromarray(img_array)
        img_name = f"sample_{i:04d}.png"
        img.save(image_dir / img_name)

        # ç”Ÿæˆä¸´åºŠæ•°æ®ï¼ˆä¸æ ‡ç­¾ç›¸å…³ï¼‰
        age = np.random.normal(60, 10) + (5 if label == 1 else 0)

        record = {
            "patient_id": f"P{i:04d}",
            "image_path": img_name,
            "age": age,
            "sex": np.random.choice(["M", "F"]),
            "diagnosis": label,
        }
        data.append(record)

    import pandas as pd
    df = pd.DataFrame(data)
    csv_path = output_dir / "dataset.csv"
    df.to_csv(csv_path, index=False)

    logger.info(f"âœ… æ•°æ®ç”Ÿæˆå®Œæˆ: {csv_path}")
    return csv_path


def train_model(model, train_loader, val_loader, device, num_epochs=5, model_name="Model"):
    """è®­ç»ƒæ¨¡å‹"""
    logger.info(f"\n{'='*60}")
    logger.info(f"è®­ç»ƒ {model_name}")
    logger.info(f"{'='*60}")

    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001)

    model = model.to(device)
    best_val_acc = 0.0

    for epoch in range(num_epochs):
        # è®­ç»ƒ
        model.train()
        train_loss = 0
        train_correct = 0
        train_total = 0

        for images, tabular, labels in train_loader:
            images = images.to(device)
            tabular = tabular.to(device)
            labels = labels.to(device)

            optimizer.zero_grad()
            outputs = model(images, tabular)

            # å¤„ç†å­—å…¸è¾“å‡º
            if isinstance(outputs, dict):
                logits = outputs.get("logits", outputs.get("output", outputs))
            else:
                logits = outputs

            loss = criterion(logits, labels)
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            _, predicted = logits.max(1)
            train_total += labels.size(0)
            train_correct += predicted.eq(labels).sum().item()

        train_acc = 100.0 * train_correct / train_total

        # éªŒè¯
        model.eval()
        val_loss = 0
        val_correct = 0
        val_total = 0

        with torch.no_grad():
            for images, tabular, labels in val_loader:
                images = images.to(device)
                tabular = tabular.to(device)
                labels = labels.to(device)

                outputs = model(images, tabular)
                if isinstance(outputs, dict):
                    logits = outputs.get("logits", outputs.get("output", outputs))
                else:
                    logits = outputs

                loss = criterion(logits, labels)

                val_loss += loss.item()
                _, predicted = logits.max(1)
                val_total += labels.size(0)
                val_correct += predicted.eq(labels).sum().item()

        val_acc = 100.0 * val_correct / val_total
        best_val_acc = max(best_val_acc, val_acc)

        logger.info(
            f"Epoch {epoch + 1}/{num_epochs} - "
            f"Train Loss: {train_loss/len(train_loader):.4f}, Train Acc: {train_acc:.2f}% | "
            f"Val Loss: {val_loss/len(val_loader):.4f}, Val Acc: {val_acc:.2f}%"
        )

    logger.info(f"âœ… {model_name} è®­ç»ƒå®Œæˆ - æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.2f}%")
    return best_val_acc


def evaluate_model(model, test_loader, device, model_name="Model"):
    """è¯„ä¼°æ¨¡å‹"""
    logger.info(f"\n{'='*60}")
    logger.info(f"è¯„ä¼° {model_name}")
    logger.info(f"{'='*60}")

    model.eval()
    all_preds = []
    all_labels = []
    all_probs = []

    with torch.no_grad():
        for images, tabular, labels in test_loader:
            images = images.to(device)
            tabular = tabular.to(device)

            outputs = model(images, tabular)
            if isinstance(outputs, dict):
                logits = outputs.get("logits", outputs.get("output", outputs))
            else:
                logits = outputs

            probs = torch.softmax(logits, dim=1)
            preds = torch.argmax(logits, dim=1)

            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
            all_probs.extend(probs[:, 1].cpu().numpy())

    # è®¡ç®—æŒ‡æ ‡
    metrics = calculate_binary_metrics(
        y_true=all_labels,
        y_pred=all_preds,
        y_prob=all_probs,
    )

    logger.info(f"æµ‹è¯•ç»“æœ:")
    logger.info(f"  - Accuracy: {metrics['accuracy']:.4f}")
    logger.info(f"  - AUC: {metrics['auc']:.4f}")
    logger.info(f"  - F1 Score: {metrics['f1']:.4f}")
    logger.info(f"  - Sensitivity: {metrics['sensitivity']:.4f}")
    logger.info(f"  - Specificity: {metrics['specificity']:.4f}")

    return metrics


def main():
    logger.info("ğŸš€ å¼€å§‹æ³¨æ„åŠ›ç›‘ç£æ•ˆæœæµ‹è¯•")
    logger.info("=" * 60)

    # 1. å‡†å¤‡æ•°æ®
    data_dir = Path("data/attention_supervision_test")
    if data_dir.exists():
        shutil.rmtree(data_dir)
    data_dir.mkdir(parents=True)

    csv_path = generate_synthetic_data(data_dir, num_samples=300)

    # 2. å‡†å¤‡æ•°æ®é›†
    logger.info("\n" + "=" * 60)
    logger.info("å‡†å¤‡æ•°æ®é›†")
    logger.info("=" * 60)

    full_dataset, _ = MedicalMultimodalDataset.from_csv(
        csv_path=str(csv_path),
        image_dir=str(data_dir / "images"),
        image_column="image_path",
        target_column="diagnosis",
        numerical_features=["age"],
        categorical_features=["sex"],
        handle_missing="fill_mean",
    )

    train_ds, val_ds, test_ds = split_dataset(
        full_dataset,
        train_ratio=0.6,
        val_ratio=0.2,
        test_ratio=0.2,
    )

    # æ·»åŠ æ•°æ®å¢å¼º
    train_ds.transform = get_train_transforms(image_size=224)
    val_ds.transform = get_val_transforms(image_size=224)
    test_ds.transform = get_val_transforms(image_size=224)

    dataloaders = create_dataloaders(
        train_dataset=train_ds,
        val_dataset=val_ds,
        test_dataset=test_ds,
        batch_size=16,
        num_workers=0,
    )

    logger.info(f"âœ… æ•°æ®é›†å‡†å¤‡å®Œæˆ")
    logger.info(f"  - è®­ç»ƒé›†: {len(train_ds)} æ ·æœ¬")
    logger.info(f"  - éªŒè¯é›†: {len(val_ds)} æ ·æœ¬")
    logger.info(f"  - æµ‹è¯•é›†: {len(test_ds)} æ ·æœ¬")

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    logger.info(f"  - è®¾å¤‡: {device}")

    # 3. åˆ›å»ºåŸºçº¿æ¨¡å‹ï¼ˆæ— æ³¨æ„åŠ›ç›‘ç£ï¼‰
    logger.info("\n" + "=" * 60)
    logger.info("åˆ›å»ºåŸºçº¿æ¨¡å‹ï¼ˆæ— æ³¨æ„åŠ›ï¼‰")
    logger.info("=" * 60)

    vision_backbone_baseline = create_vision_backbone(
        backbone_name="resnet18",
        pretrained=True,
        feature_dim=128,
    )

    tabular_backbone_baseline = create_tabular_backbone(
        input_dim=train_ds.get_tabular_dim(),
        output_dim=16,
        hidden_dims=[32, 32],
    )

    fusion_module_baseline = create_fusion_module(
        fusion_type="concatenate",
        vision_dim=128,
        tabular_dim=16,
        output_dim=64,
    )

    model_baseline = MultiModalFusionModel(
        vision_backbone=vision_backbone_baseline,
        tabular_backbone=tabular_backbone_baseline,
        fusion_module=fusion_module_baseline,
        num_classes=2,
    )

    logger.info(f"âœ… åŸºçº¿æ¨¡å‹åˆ›å»ºå®Œæˆ")

    # 4. åˆ›å»ºæ³¨æ„åŠ›ç›‘ç£æ¨¡å‹
    logger.info("\n" + "=" * 60)
    logger.info("åˆ›å»ºæ³¨æ„åŠ›ç›‘ç£æ¨¡å‹ï¼ˆCBAMï¼‰")
    logger.info("=" * 60)

    vision_backbone_attention = create_vision_backbone(
        backbone_name="resnet18",
        pretrained=True,
        feature_dim=128,
        attention_type="cbam",  # ä½¿ç”¨ CBAM æ³¨æ„åŠ›
    )

    tabular_backbone_attention = create_tabular_backbone(
        input_dim=train_ds.get_tabular_dim(),
        output_dim=16,
        hidden_dims=[32, 32],
    )

    fusion_module_attention = create_fusion_module(
        fusion_type="concatenate",
        vision_dim=128,
        tabular_dim=16,
        output_dim=64,
    )

    model_attention = MultiModalFusionModel(
        vision_backbone=vision_backbone_attention,
        tabular_backbone=tabular_backbone_attention,
        fusion_module=fusion_module_attention,
        num_classes=2,
    )

    logger.info(f"âœ… æ³¨æ„åŠ›æ¨¡å‹åˆ›å»ºå®Œæˆ")

    # 5. è®­ç»ƒåŸºçº¿æ¨¡å‹
    best_val_acc_baseline = train_model(
        model_baseline,
        dataloaders["train"],
        dataloaders["val"],
        device,
        num_epochs=10,
        model_name="åŸºçº¿æ¨¡å‹ï¼ˆæ— æ³¨æ„åŠ›ï¼‰",
    )

    # 6. è®­ç»ƒæ³¨æ„åŠ›æ¨¡å‹
    best_val_acc_attention = train_model(
        model_attention,
        dataloaders["train"],
        dataloaders["val"],
        device,
        num_epochs=10,
        model_name="æ³¨æ„åŠ›ç›‘ç£æ¨¡å‹ï¼ˆCBAMï¼‰",
    )

    # 7. è¯„ä¼°åŸºçº¿æ¨¡å‹
    metrics_baseline = evaluate_model(
        model_baseline,
        dataloaders["test"],
        device,
        model_name="åŸºçº¿æ¨¡å‹ï¼ˆæ— æ³¨æ„åŠ›ï¼‰",
    )

    # 8. è¯„ä¼°æ³¨æ„åŠ›æ¨¡å‹
    metrics_attention = evaluate_model(
        model_attention,
        dataloaders["test"],
        device,
        model_name="æ³¨æ„åŠ›ç›‘ç£æ¨¡å‹ï¼ˆCBAMï¼‰",
    )

    # 9. å¯¹æ¯”ç»“æœ
    logger.info("\n" + "=" * 60)
    logger.info("ğŸ“Š å¯¹æ¯”ç»“æœ")
    logger.info("=" * 60)

    logger.info(f"\néªŒè¯é›†æœ€ä½³å‡†ç¡®ç‡:")
    logger.info(f"  åŸºçº¿æ¨¡å‹: {best_val_acc_baseline:.2f}%")
    logger.info(f"  æ³¨æ„åŠ›æ¨¡å‹: {best_val_acc_attention:.2f}%")
    logger.info(f"  æå‡: {best_val_acc_attention - best_val_acc_baseline:+.2f}%")

    logger.info(f"\næµ‹è¯•é›†å‡†ç¡®ç‡:")
    logger.info(f"  åŸºçº¿æ¨¡å‹: {metrics_baseline['accuracy']:.4f}")
    logger.info(f"  æ³¨æ„åŠ›æ¨¡å‹: {metrics_attention['accuracy']:.4f}")
    logger.info(f"  æå‡: {metrics_attention['accuracy'] - metrics_baseline['accuracy']:+.4f}")

    logger.info(f"\næµ‹è¯•é›† AUC:")
    logger.info(f"  åŸºçº¿æ¨¡å‹: {metrics_baseline['auc']:.4f}")
    logger.info(f"  æ³¨æ„åŠ›æ¨¡å‹: {metrics_attention['auc']:.4f}")
    logger.info(f"  æå‡: {metrics_attention['auc'] - metrics_baseline['auc']:+.4f}")

    logger.info(f"\næµ‹è¯•é›† F1 Score:")
    logger.info(f"  åŸºçº¿æ¨¡å‹: {metrics_baseline['f1']:.4f}")
    logger.info(f"  æ³¨æ„åŠ›æ¨¡å‹: {metrics_attention['f1']:.4f}")
    logger.info(f"  æå‡: {metrics_attention['f1'] - metrics_baseline['f1']:+.4f}")

    # 10. ç»“è®º
    logger.info("\n" + "=" * 60)
    logger.info("ğŸ’¡ ç»“è®º")
    logger.info("=" * 60)

    improvement = metrics_attention['accuracy'] - metrics_baseline['accuracy']

    if improvement > 0.02:  # æå‡è¶…è¿‡ 2%
        logger.info("âœ… æ³¨æ„åŠ›ç›‘ç£æœ‰æ˜æ˜¾æ•ˆæœï¼Œå»ºè®®ä¿ç•™")
        logger.info(f"   å‡†ç¡®ç‡æå‡: {improvement:.4f} ({improvement*100:.2f}%)")
    elif improvement > 0:
        logger.info("âš ï¸ æ³¨æ„åŠ›ç›‘ç£æœ‰è½»å¾®æå‡ï¼Œä½†ä¸æ˜æ˜¾")
        logger.info(f"   å‡†ç¡®ç‡æå‡: {improvement:.4f} ({improvement*100:.2f}%)")
        logger.info("   å»ºè®®ï¼šåœ¨çœŸå®æ•°æ®ä¸Šæµ‹è¯•åå†å†³å®š")
    else:
        logger.info("âŒ æ³¨æ„åŠ›ç›‘ç£æ²¡æœ‰æå‡ï¼Œç”šè‡³å¯èƒ½é™ä½æ€§èƒ½")
        logger.info(f"   å‡†ç¡®ç‡å˜åŒ–: {improvement:.4f} ({improvement*100:.2f}%)")
        logger.info("   å»ºè®®ï¼šåˆ é™¤æ³¨æ„åŠ›ç›‘ç£æ¨¡å—ï¼ˆ2,678 è¡Œä»£ç ï¼‰")

    logger.info("\næ³¨æ„ï¼šè¿™æ˜¯åˆæˆæ•°æ®æµ‹è¯•ï¼ŒçœŸå®æ•°æ®å¯èƒ½æœ‰ä¸åŒç»“æœ")


if __name__ == "__main__":
    main()
