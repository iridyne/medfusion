#!/usr/bin/env python
"""
è¿è¡Œ MedFusion åŸºå‡†æµ‹è¯•

æµ‹è¯•å…³é”®ç»„ä»¶çš„æ€§èƒ½ï¼Œç”ŸæˆåŸºçº¿æ•°æ®ã€‚
"""

import argparse
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))


def benchmark_data_loading():
    """åŸºå‡†æµ‹è¯•ï¼šæ•°æ®åŠ è½½"""
    print("\n" + "=" * 60)
    print("æ•°æ®åŠ è½½åŸºå‡†æµ‹è¯•")
    print("=" * 60)

    import time

    # æ¨¡æ‹Ÿæ•°æ®åŠ è½½
    def load_data_no_cache():
        """æ— ç¼“å­˜çš„æ•°æ®åŠ è½½"""
        time.sleep(0.001)  # æ¨¡æ‹Ÿ I/O
        return list(range(100))

    def load_data_with_cache():
        """æœ‰ç¼“å­˜çš„æ•°æ®åŠ è½½"""
        cache = {}

        def load(idx):
            if idx in cache:
                return cache[idx]
            time.sleep(0.001)
            data = list(range(100))
            cache[idx] = data
            return data

        return load

    # æµ‹è¯•æ— ç¼“å­˜
    print("\n1. æ— ç¼“å­˜:")
    start = time.time()
    for _ in range(100):
        load_data_no_cache()
    time_no_cache = time.time() - start
    throughput_no_cache = 100 / time_no_cache
    print(f"   è€—æ—¶: {time_no_cache:.3f}s")
    print(f"   ååé‡: {throughput_no_cache:.1f} samples/s")

    # æµ‹è¯•æœ‰ç¼“å­˜
    print("\n2. æœ‰ç¼“å­˜:")
    loader = load_data_with_cache()
    start = time.time()
    for i in range(100):
        loader(i % 10)  # é‡å¤è®¿é—® 10 ä¸ªæ ·æœ¬
    time_with_cache = time.time() - start
    throughput_with_cache = 100 / time_with_cache
    print(f"   è€—æ—¶: {time_with_cache:.3f}s")
    print(f"   ååé‡: {throughput_with_cache:.1f} samples/s")

    # åŠ é€Ÿæ¯”
    speedup = throughput_with_cache / throughput_no_cache
    print(f"\n3. åŠ é€Ÿæ¯”: {speedup:.1f}x")

    return {
        "no_cache": {
            "duration": time_no_cache,
            "throughput": throughput_no_cache,
        },
        "with_cache": {
            "duration": time_with_cache,
            "throughput": throughput_with_cache,
        },
        "speedup": speedup,
    }


def benchmark_fusion_strategies():
    """åŸºå‡†æµ‹è¯•ï¼šèåˆç­–ç•¥"""
    print("\n" + "=" * 60)
    print("èåˆç­–ç•¥åŸºå‡†æµ‹è¯•")
    print("=" * 60)

    import time

    # æ¨¡æ‹Ÿä¸åŒçš„èåˆç­–ç•¥
    def concatenate_fusion(v, t):
        """æ‹¼æ¥èåˆ"""
        return v + t

    def gated_fusion(v, t):
        """é—¨æ§èåˆ"""
        alpha = 0.5
        beta = 0.5
        return alpha * v + beta * t

    def attention_fusion(v, t):
        """æ³¨æ„åŠ›èåˆ"""
        # ç®€åŒ–çš„æ³¨æ„åŠ›è®¡ç®—
        weight_v = v / (v + t + 1e-8)
        weight_t = t / (v + t + 1e-8)
        return weight_v * v + weight_t * t

    strategies = {
        "Concatenate": concatenate_fusion,
        "Gated": gated_fusion,
        "Attention": attention_fusion,
    }

    results = {}

    for name, func in strategies.items():
        print(f"\n{name} Fusion:")

        # æµ‹è¯•
        start = time.time()
        for _ in range(10000):
            func(1.0, 2.0)
        duration = time.time() - start
        throughput = 10000 / duration

        print(f"   è€—æ—¶: {duration:.3f}s")
        print(f"   ååé‡: {throughput:.1f} ops/s")

        results[name] = {
            "duration": duration,
            "throughput": throughput,
        }

    return results


def benchmark_aggregators():
    """åŸºå‡†æµ‹è¯•ï¼šèšåˆå™¨"""
    print("\n" + "=" * 60)
    print("èšåˆå™¨åŸºå‡†æµ‹è¯•")
    print("=" * 60)

    import time

    # æ¨¡æ‹Ÿæ•°æ®
    data = [[i + j for j in range(10)] for i in range(100)]

    # ä¸åŒçš„èšåˆç­–ç•¥
    def mean_pooling(instances):
        """å‡å€¼æ± åŒ–"""
        return [sum(inst) / len(inst) for inst in instances]

    def max_pooling(instances):
        """æœ€å¤§æ± åŒ–"""
        return [max(inst) for inst in instances]

    def attention_pooling(instances):
        """æ³¨æ„åŠ›æ± åŒ–"""
        # ç®€åŒ–çš„æ³¨æ„åŠ›
        weights = [[1.0 / len(inst)] * len(inst) for inst in instances]
        return [
            sum(w * v for w, v in zip(weight, inst))
            for weight, inst in zip(weights, instances)
        ]

    aggregators = {
        "Mean": mean_pooling,
        "Max": max_pooling,
        "Attention": attention_pooling,
    }

    results = {}

    for name, func in aggregators.items():
        print(f"\n{name} Pooling:")

        # æµ‹è¯•
        start = time.time()
        for _ in range(1000):
            func(data)
        duration = time.time() - start
        throughput = 1000 / duration

        print(f"   è€—æ—¶: {duration:.3f}s")
        print(f"   ååé‡: {throughput:.1f} ops/s")

        results[name] = {
            "duration": duration,
            "throughput": throughput,
        }

    return results


def benchmark_preprocessing():
    """åŸºå‡†æµ‹è¯•ï¼šé¢„å¤„ç†"""
    print("\n" + "=" * 60)
    print("é¢„å¤„ç†åŸºå‡†æµ‹è¯•")
    print("=" * 60)

    import time

    # æ¨¡æ‹Ÿå›¾åƒæ•°æ®
    image = [[i + j for j in range(224)] for i in range(224)]

    # ä¸åŒçš„é¢„å¤„ç†æ“ä½œ
    def resize(img):
        """è°ƒæ•´å¤§å°ï¼ˆç®€åŒ–ï¼‰"""
        return [[img[i][j] for j in range(0, 224, 2)] for i in range(0, 224, 2)]

    def normalize(img):
        """å½’ä¸€åŒ–"""
        mean = sum(sum(row) for row in img) / (224 * 224)
        return [[pixel - mean for pixel in row] for row in img]

    def augment(img):
        """æ•°æ®å¢å¼ºï¼ˆç®€åŒ–ï¼‰"""
        # æ°´å¹³ç¿»è½¬
        return [row[::-1] for row in img]

    operations = {
        "Resize": resize,
        "Normalize": normalize,
        "Augment": augment,
    }

    results = {}

    for name, func in operations.items():
        print(f"\n{name}:")

        # æµ‹è¯•
        start = time.time()
        for _ in range(1000):
            func(image)
        duration = time.time() - start
        throughput = 1000 / duration

        print(f"   è€—æ—¶: {duration:.3f}s")
        print(f"   ååé‡: {throughput:.1f} ops/s")

        results[name] = {
            "duration": duration,
            "throughput": throughput,
        }

    return results


def save_results(results, output_file):
    """ä¿å­˜ç»“æœåˆ°æ–‡ä»¶"""
    import json

    output_path = Path(output_file)
    output_path.parent.mkdir(parents=True, exist_ok=True)

    with open(output_path, "w") as f:
        json.dump(results, f, indent=2)

    print(f"\nç»“æœå·²ä¿å­˜åˆ°: {output_path}")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="è¿è¡Œ MedFusion åŸºå‡†æµ‹è¯•")
    parser.add_argument(
        "--output",
        type=str,
        default="benchmarks/baseline.json",
        help="è¾“å‡ºæ–‡ä»¶è·¯å¾„",
    )
    parser.add_argument(
        "--tests",
        nargs="+",
        choices=["data", "fusion", "aggregator", "preprocess", "all"],
        default=["all"],
        help="è¦è¿è¡Œçš„æµ‹è¯•",
    )

    args = parser.parse_args()

    print("\n" + "=" * 60)
    print("MedFusion æ€§èƒ½åŸºå‡†æµ‹è¯•")
    print("=" * 60)

    all_results = {}

    # è¿è¡Œæµ‹è¯•
    if "all" in args.tests or "data" in args.tests:
        all_results["data_loading"] = benchmark_data_loading()

    if "all" in args.tests or "fusion" in args.tests:
        all_results["fusion_strategies"] = benchmark_fusion_strategies()

    if "all" in args.tests or "aggregator" in args.tests:
        all_results["aggregators"] = benchmark_aggregators()

    if "all" in args.tests or "preprocess" in args.tests:
        all_results["preprocessing"] = benchmark_preprocessing()

    # ä¿å­˜ç»“æœ
    save_results(all_results, args.output)

    print("\n" + "=" * 60)
    print("åŸºå‡†æµ‹è¯•å®Œæˆï¼")
    print("=" * 60)

    print("\nğŸ’¡ æç¤º:")
    print("  â€¢ ä½¿ç”¨è¿™äº›ç»“æœä½œä¸ºæ€§èƒ½åŸºçº¿")
    print("  â€¢ åœ¨ä»£ç å˜æ›´åé‡æ–°è¿è¡Œæµ‹è¯•")
    print("  â€¢ æ¯”è¾ƒç»“æœä»¥æ£€æµ‹æ€§èƒ½å›å½’")
    print("  â€¢ é›†æˆåˆ° CI/CD æµç¨‹ä¸­")


if __name__ == "__main__":
    main()
